"""
í™œë™ ë¦¬í¬íŠ¸ - ì‹¤í—˜ìš© ëŒ€ì‹œë³´ë“œ

ì‹¤í—˜ ê¸°ëŠ¥:
- ì¼ë³„/ì£¼ê°„ ë¦¬í¬íŠ¸ íƒ€ì… ì„ íƒ
- ë‹¤ì–‘í•œ LLM ëª¨ë¸ ì„ íƒ ë° ë¹„êµ
- í”„ë¼ì´ë²„ì‹œ í•„í„° on/off í† ê¸€
- ëª¨ë¸ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ (í† í°, ì‹œê°„, ë¹„ìš©)
- ì „ì²´ ë°ì´í„° ì ‘ê·¼ ê°€ëŠ¥

ì™¼ìª½: Interactive ì‹œê°í™” ì¸ì‚¬ì´íŠ¸
ì˜¤ë¥¸ìª½: ì‹¤í—˜ìš© LLM í”¼ë“œë°±
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parents[3]
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import time

from llm_engineering.domain.cleaned_documents import CleanedCalendarDocument
from llm_engineering.application.feedback.daily.generator import DailyFeedbackGenerator
from llm_engineering.application.feedback.weekly.generator import WeeklyFeedbackGenerator
from llm_engineering.application.feedback.monthly.generator import MonthlyFeedbackGenerator
from llm_engineering.application.visualization.daily_report_interactive import (
    format_duration,
    plot_agency_pie_chart_interactive,
    plot_category_distribution_interactive,
    plot_sleep_breakdown_interactive,
    plot_work_by_event_interactive,
    plot_learning_by_event_interactive,
    plot_recharge_by_event_interactive,
    plot_drain_by_event_interactive,
    plot_maintenance_by_event_interactive,
    plot_relationship_by_agency_interactive,
)
from llm_engineering.application.visualization.privacy_utils import (
    apply_public_privacy_filter,
)


# Tooltip ìŠ¤íƒ€ì¼ ì •ì˜
TOOLTIP_CSS = """
<style>
.chart-title-tooltip {
    position: relative;
    display: inline-block;
    cursor: help;
    font-size: 1.5rem;
    font-weight: 600;
    margin-bottom: 0.5rem;
}

.chart-title-tooltip .tooltiptext {
    visibility: hidden;
    width: 300px;
    background-color: #262730;
    color: #fafafa;
    text-align: left;
    padding: 10px;
    border-radius: 6px;
    border: 1px solid #464646;

    position: absolute;
    z-index: 1000;
    bottom: 125%;
    left: 50%;
    margin-left: -150px;

    opacity: 0;
    transition: opacity 0.3s;

    font-size: 0.875rem;
    font-weight: normal;
}

.chart-title-tooltip:hover .tooltiptext {
    visibility: visible;
    opacity: 1;
}

.chart-title-tooltip .tooltiptext::after {
    content: "";
    position: absolute;
    top: 100%;
    left: 50%;
    margin-left: -5px;
    border-width: 5px;
    border-style: solid;
    border-color: #464646 transparent transparent transparent;
}
</style>
"""


# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
OPENAI_MODELS = {
    # GPT-5 Series (ìµœì‹ )
    "gpt-5.1": {"name": "GPT-5.1 (ìµœì‹ )", "cost_per_1k_input": 10.0, "cost_per_1k_output": 30.0},
    "gpt-5": {"name": "GPT-5", "cost_per_1k_input": 8.0, "cost_per_1k_output": 24.0},
    "gpt-5-pro": {"name": "GPT-5 Pro", "cost_per_1k_input": 12.0, "cost_per_1k_output": 36.0},
    "gpt-5-mini": {"name": "GPT-5 Mini", "cost_per_1k_input": 1.0, "cost_per_1k_output": 3.0},
    "gpt-5-nano": {"name": "GPT-5 Nano", "cost_per_1k_input": 0.5, "cost_per_1k_output": 1.5},

    # GPT-4.1 Series
    "gpt-4.1": {"name": "GPT-4.1", "cost_per_1k_input": 7.0, "cost_per_1k_output": 21.0},
    "gpt-4.1-mini": {"name": "GPT-4.1 Mini", "cost_per_1k_input": 0.8, "cost_per_1k_output": 2.4},
    "gpt-4.1-nano": {"name": "GPT-4.1 Nano", "cost_per_1k_input": 0.4, "cost_per_1k_output": 1.2},

    # GPT-4o Series
    "gpt-4o": {"name": "GPT-4o", "cost_per_1k_input": 5.0, "cost_per_1k_output": 15.0},
    "gpt-4o-mini": {"name": "GPT-4o Mini", "cost_per_1k_input": 0.15, "cost_per_1k_output": 0.60},

    # GPT-4 Series
    "gpt-4-turbo": {"name": "GPT-4 Turbo", "cost_per_1k_input": 10.0, "cost_per_1k_output": 30.0},
    "gpt-4": {"name": "GPT-4", "cost_per_1k_input": 30.0, "cost_per_1k_output": 60.0},

    # GPT-3.5 Series
    "gpt-3.5-turbo": {"name": "GPT-3.5 Turbo", "cost_per_1k_input": 0.50, "cost_per_1k_output": 1.50},
}

GEMINI_MODELS = {
    # Gemini 2.5 Series (ìµœì‹ )
    "gemini-2.5-pro": {"name": "Gemini 2.5 Pro (ìµœì‹ )", "cost_per_1k_input": 1.25, "cost_per_1k_output": 5.0},
    "gemini-2.5-flash": {"name": "Gemini 2.5 Flash", "cost_per_1k_input": 0.075, "cost_per_1k_output": 0.30},
    "gemini-2.5-flash-lite": {"name": "Gemini 2.5 Flash-Lite", "cost_per_1k_input": 0.0375, "cost_per_1k_output": 0.15},

    # Gemini 2.0 Series
    "gemini-2.0-flash-exp": {"name": "Gemini 2.0 Flash Exp (ë¬´ë£Œ)", "cost_per_1k_input": 0.0, "cost_per_1k_output": 0.0},
    "gemini-2.0-flash": {"name": "Gemini 2.0 Flash", "cost_per_1k_input": 0.05, "cost_per_1k_output": 0.20},

    # Gemini 1.5 Series
    "gemini-1.5-pro": {"name": "Gemini 1.5 Pro", "cost_per_1k_input": 1.25, "cost_per_1k_output": 5.0},
    "gemini-1.5-flash": {"name": "Gemini 1.5 Flash", "cost_per_1k_input": 0.075, "cost_per_1k_output": 0.30},
}

# ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼
PROMPT_STYLES = [
    "original",
    "minimal",
    "coach",
    "scientist",
    "cbt",
    "narrative",
    "dashboard",
    "metacognitive",
    "public"
]


def show_section_title_with_tooltip(title: str, tooltip: str):
    """
    í˜¸ë²„ ì‹œ íˆ´íŒì´ ë‚˜íƒ€ë‚˜ëŠ” ì„¹ì…˜ ì œëª© í‘œì‹œ

    Args:
        title: ì„¹ì…˜ ì œëª©
        tooltip: í˜¸ë²„ ì‹œ ë‚˜íƒ€ë‚  íˆ´íŒ í…ìŠ¤íŠ¸
    """
    st.markdown(TOOLTIP_CSS, unsafe_allow_html=True)
    st.markdown(f"""
    <div class="chart-title-tooltip">
        {title}
        <span class="tooltiptext">{tooltip}</span>
    </div>
    """, unsafe_allow_html=True)


def get_weekday_korean(date_str: str) -> str:
    """
    ë‚ ì§œ ë¬¸ìì—´ì—ì„œ í•œê¸€ ìš”ì¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        date_str: YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´

    Returns:
        í•œê¸€ ìš”ì¼ (ì›”, í™”, ìˆ˜, ëª©, ê¸ˆ, í† , ì¼)
    """
    weekday_map = {
        0: 'ì›”', 1: 'í™”', 2: 'ìˆ˜', 3: 'ëª©',
        4: 'ê¸ˆ', 5: 'í† ', 6: 'ì¼'
    }
    date_obj = pd.to_datetime(date_str)
    return weekday_map[date_obj.weekday()]


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì¼ë³„ í™œë™ ë¦¬í¬íŠ¸ (ì‹¤í—˜)",
    page_icon="ğŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded",
)


def load_daily_data(date_str: str, apply_privacy_filter: bool = False) -> pd.DataFrame:
    """
    íŠ¹ì • ë‚ ì§œì˜ CleanedCalendarDocumentë¥¼ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜.

    Args:
        date_str: ë‚ ì§œ (YYYY-MM-DD)
        apply_privacy_filter: í”„ë¼ì´ë²„ì‹œ í•„í„° ì ìš© ì—¬ë¶€
    """
    try:
        docs = list(CleanedCalendarDocument.bulk_find(ref_date=date_str))

        if not docs:
            return None

        data = []
        for doc in docs:
            metadata = doc.metadata
            data.append({
                'original_id': str(doc.original_id),
                'start_datetime': pd.to_datetime(metadata.get('start_datetime')),
                'end_datetime': pd.to_datetime(metadata.get('end_datetime')),
                'duration_minutes': metadata.get('duration_minutes', 0),
                'category_name': metadata.get('category_name'),
                'calendar_name': metadata.get('category_name'),
                'event_name': metadata.get('event_name'),
                'notes': metadata.get('notes', ''),
                'sub_category': metadata.get('sub_category', ''),
                'learning_method': metadata.get('learning_method'),
                'learning_target': metadata.get('learning_target'),
                'work_tags': metadata.get('work_tags', []),
                'exercise_type': metadata.get('exercise_type'),
                'is_risky_recharger': metadata.get('is_risky_recharger', False),
                'has_relationship_tag': metadata.get('has_relationship_tag', False),
                'has_emotion_event': metadata.get('has_emotion_event', False),
            })

        df = pd.DataFrame(data)
        df = df.sort_values('start_datetime').reset_index(drop=True)

        # í”„ë¼ì´ë²„ì‹œ í•„í„° ì ìš© (ì„ íƒì )
        if apply_privacy_filter:
            df = apply_public_privacy_filter(
                df,
                days=7,
                ref_date=date_str,
                mask_notes=True,
                anonymize_names=True
            )

        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def load_weekly_data(start_date: str, end_date: str, apply_privacy_filter: bool = False) -> pd.DataFrame:
    """
    íŠ¹ì • ê¸°ê°„ì˜ CleanedCalendarDocumentë¥¼ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜.

    Args:
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
        apply_privacy_filter: í”„ë¼ì´ë²„ì‹œ í•„í„° ì ìš© ì—¬ë¶€
    """
    try:
        # ë‚ ì§œ ë²”ìœ„ ë‚´ ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
        all_docs = []
        current_date = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")

        while current_date <= end_dt:
            date_str = current_date.strftime("%Y-%m-%d")
            docs = list(CleanedCalendarDocument.bulk_find(ref_date=date_str))
            all_docs.extend(docs)
            current_date += timedelta(days=1)

        if not all_docs:
            return None

        data = []
        for doc in all_docs:
            metadata = doc.metadata
            data.append({
                'original_id': str(doc.original_id),
                'ref_date': doc.ref_date,
                'start_datetime': pd.to_datetime(metadata.get('start_datetime')),
                'end_datetime': pd.to_datetime(metadata.get('end_datetime')),
                'duration_minutes': metadata.get('duration_minutes', 0),
                'category_name': metadata.get('category_name'),
                'calendar_name': metadata.get('category_name'),
                'event_name': metadata.get('event_name'),
                'notes': metadata.get('notes', ''),
                'sub_category': metadata.get('sub_category', ''),
                'learning_method': metadata.get('learning_method'),
                'learning_target': metadata.get('learning_target'),
                'work_tags': metadata.get('work_tags', []),
                'exercise_type': metadata.get('exercise_type'),
                'is_risky_recharger': metadata.get('is_risky_recharger', False),
                'has_relationship_tag': metadata.get('has_relationship_tag', False),
                'has_emotion_event': metadata.get('has_emotion_event', False),
            })

        df = pd.DataFrame(data)
        df = df.sort_values('start_datetime').reset_index(drop=True)

        # í”„ë¼ì´ë²„ì‹œ í•„í„° ì ìš© (ì„ íƒì )
        if apply_privacy_filter:
            df = apply_public_privacy_filter(
                df,
                days=7,
                ref_date=end_date,
                mask_notes=True,
                anonymize_names=True
            )

        return df
    except Exception as e:
        st.error(f"ì£¼ê°„ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def load_monthly_data(year: int, month: int, apply_privacy_filter: bool = False) -> pd.DataFrame:
    """
    íŠ¹ì • ì›”ì˜ CleanedCalendarDocumentë¥¼ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜.

    Args:
        year: ì—°ë„ (YYYY)
        month: ì›” (1-12)
        apply_privacy_filter: í”„ë¼ì´ë²„ì‹œ í•„í„° ì ìš© ì—¬ë¶€
    """
    try:
        # ì›”ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°
        start_date = datetime(year, month, 1)
        if month == 12:
            end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
        else:
            end_date = datetime(year, month + 1, 1) - timedelta(days=1)

        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")

        # ë‚ ì§œ ë²”ìœ„ ë‚´ ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
        all_docs = []
        current_date = start_date

        while current_date <= end_date:
            date_str = current_date.strftime("%Y-%m-%d")
            docs = list(CleanedCalendarDocument.bulk_find(ref_date=date_str))
            all_docs.extend(docs)
            current_date += timedelta(days=1)

        if not all_docs:
            return None

        data = []
        for doc in all_docs:
            metadata = doc.metadata
            data.append({
                'original_id': str(doc.original_id),
                'ref_date': doc.ref_date,
                'start_datetime': pd.to_datetime(metadata.get('start_datetime')),
                'end_datetime': pd.to_datetime(metadata.get('end_datetime')),
                'duration_minutes': metadata.get('duration_minutes', 0),
                'category_name': metadata.get('category_name'),
                'calendar_name': metadata.get('category_name'),
                'event_name': metadata.get('event_name'),
                'notes': metadata.get('notes', ''),
                'sub_category': metadata.get('sub_category', ''),
                'learning_method': metadata.get('learning_method'),
                'learning_target': metadata.get('learning_target'),
                'work_tags': metadata.get('work_tags', []),
                'exercise_type': metadata.get('exercise_type'),
                'is_risky_recharger': metadata.get('is_risky_recharger', False),
                'has_relationship_tag': metadata.get('has_relationship_tag', False),
                'has_emotion_event': metadata.get('has_emotion_event', False),
            })

        df = pd.DataFrame(data)
        df = df.sort_values('start_datetime').reset_index(drop=True)

        # í”„ë¼ì´ë²„ì‹œ í•„í„° ì ìš© (ì„ íƒì )
        if apply_privacy_filter:
            df = apply_public_privacy_filter(
                df,
                days=30,
                ref_date=end_str,
                mask_notes=True,
                anonymize_names=True
            )

        return df
    except Exception as e:
        st.error(f"ì›”ê°„ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def show_statistics(df: pd.DataFrame, target_date: str, is_weekly: bool = False, start_date: str = None, end_date: str = None):
    """ì „ì²´ í†µê³„ í‘œì‹œ"""
    if is_weekly and start_date and end_date:
        st.subheader(f"ğŸ“Š ì£¼ê°„ í†µê³„: {start_date} ~ {end_date}")
    else:
        weekday = get_weekday_korean(target_date)
        st.subheader(f"ğŸ“Š {target_date} ({weekday}) ì „ì²´ í†µê³„")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("ì´ ê¸°ë¡ ì‹œê°„", format_duration(df['duration_minutes'].sum()))

    with col2:
        st.metric("ì´ í™œë™ ìˆ˜", f"{len(df)}ê°œ")

    with col3:
        st.metric("#ì¸ê°„ê´€ê³„", f"{df['has_relationship_tag'].sum()}ê°œ")

    with col4:
        st.metric("#ì¦‰ì‹œë§Œì¡±", f"{df['is_risky_recharger'].sum()}ê°œ")


def show_agency_pie_chart(df: pd.DataFrame):
    """Agency íŒŒì´ì°¨íŠ¸ í‘œì‹œ (Interactive)"""
    show_section_title_with_tooltip(
        "ğŸ¯ Agency íŒŒì´ì°¨íŠ¸",
        "ğŸ’¡ Tip: í˜¸ë²„í•˜ë©´ ì‹¤ì œ ì˜ì—­ë³„ í•©ê³„ ì‹œê°„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )

    fig = plot_agency_pie_chart_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  Agency ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def show_category_distribution(df: pd.DataFrame):
    """ì¹´í…Œê³ ë¦¬ë³„ ì‹œê°„ ë¶„í¬ í‘œì‹œ (Interactive)"""
    show_section_title_with_tooltip(
        "ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì‹œê°„ ë¶„í¬",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ í•˜ë£¨ ê¸°ì¤€ í¼ì„¼í‹°ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ê³¼ë„í•˜ê²Œ íšŒë³µì— ë§ì´ ì‚¬ìš©ë ê²½ìš° ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤"
    )

    fig = plot_category_distribution_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def show_sleep_breakdown(df: pd.DataFrame):
    """ìˆ˜ë©´ ìƒì„¸ ë¶„ì„ í‘œì‹œ (Interactive)"""
    show_section_title_with_tooltip(
        "ğŸ˜´ ìˆ˜ë©´ ìƒì„¸ ë¶„ì„",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ê° ìˆ˜ë©´ ì´ë²¤íŠ¸ì˜ ë©”ëª¨ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )

    fig = plot_sleep_breakdown_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  ìˆ˜ë©´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def show_five_areas_analysis(df: pd.DataFrame):
    """5ê°œ ì˜ì—­ ìƒì„¸ ë¶„ì„ (Interactive Plotly)"""

    # 1. ì¼/ìƒì‚°
    show_section_title_with_tooltip(
        "ğŸ’¼ ì¼/ìƒì‚°",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ë©”ëª¨ì™€ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    fig = plot_work_by_event_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  ì¼/ìƒì‚° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 2. í•™ìŠµ/ì„±ì¥
    show_section_title_with_tooltip(
        "ğŸ“š í•™ìŠµ/ì„±ì¥ ",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ë©”ëª¨ì™€ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    fig = plot_learning_by_event_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  í•™ìŠµ/ì„±ì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 3. ì¬ì¶©ì „ í™œë™
    show_section_title_with_tooltip(
        "ğŸŒ´ íœ´ì‹/íšŒë³µ ",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ë©”ëª¨ì™€ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    fig = plot_recharge_by_event_interactive(df, top_n=15, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  ì¬ì¶©ì „ í™œë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 4. Drain
    show_section_title_with_tooltip(
        "âš ï¸ Drain",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ë©”ëª¨ì™€ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    fig = plot_drain_by_event_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  Drain ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 5. ì¼ìƒ ê´€ë¦¬

    show_section_title_with_tooltip(
        "ğŸ  ìœ ì§€/ì •ë¦¬",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ë©”ëª¨ì™€ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    fig = plot_maintenance_by_event_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  ìœ ì§€/ì •ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # 6. #ì¸ê°„ê´€ê³„ íƒœê·¸ - Agencyë³„
    show_section_title_with_tooltip(
        "ğŸ‘¥ ì¸ê°„ê´€ê³„ - Agencyë³„ ë¶„í¬",
        "ğŸ’¡ Tip: ë°”ë¥¼ í˜¸ë²„í•˜ë©´ ë©”ëª¨ì™€ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
    )
    fig = plot_relationship_by_agency_interactive(df, show_title=False)
    if fig:
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("âš ï¸  #ì¸ê°„ê´€ê³„ íƒœê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


def generate_feedback_with_metrics(
    date_str: str,
    provider: str,
    model_id: str,
    temperature: float,
    prompt_style: str,
) -> tuple[str, dict]:
    """
    í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        date_str: ë‚ ì§œ
        provider: ëª¨ë¸ ì œê³µì (OpenAI / Gemini)
        model_id: ëª¨ë¸ ID
        temperature: ì˜¨ë„
        prompt_style: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼

    Returns:
        (í”¼ë“œë°± ë‚´ìš©, ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬)
    """
    try:
        # Providerì— ë”°ë¼ DailyFeedbackGenerator ë˜ëŠ” ì»¤ìŠ¤í…€ ìƒì„±
        if provider == "OpenAI":
            # GPT-5, GPT-4.1 ì‹œë¦¬ì¦ˆëŠ” temperatureë¥¼ ì§€ì›í•˜ì§€ ì•Šê±°ë‚˜ ê¸°ë³¸ê°’(1.0)ë§Œ ì§€ì›
            # ì´ëŸ° ëª¨ë¸ë“¤ì€ temperatureë¥¼ Noneìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
            temperature_restricted_models = [
                "gpt-5", "gpt-5.1", "gpt-5-pro", "gpt-5-mini", "gpt-5-nano",
                "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"
            ]

            if model_id in temperature_restricted_models:
                # temperatureë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’ ì‚¬ìš©)
                generator = DailyFeedbackGenerator(
                    model_id=model_id,
                    temperature=1.0,  # ê¸°ë³¸ê°’ ì‚¬ìš©
                    prompt_style=prompt_style
                )
                actual_temperature = 1.0
            else:
                generator = DailyFeedbackGenerator(
                    model_id=model_id,
                    temperature=temperature,
                    prompt_style=prompt_style
                )
                actual_temperature = temperature

            start_time = time.time()

            feedback_content = generator.generate(
                target_date=date_str,
                include_previous=True,
                include_next=True,
                save_to_db=False  # ì‹¤í—˜ìš©ì´ë¯€ë¡œ DBì— ì €ì¥ ì•ˆ í•¨
            )

            end_time = time.time()
            generation_time = end_time - start_time

            # ë©”íŠ¸ë¦­ ê³„ì‚° (ëŒ€ëµì )
            input_tokens = len(feedback_content) // 4  # ëŒ€ëµì  ì¶”ì •
            output_tokens = len(feedback_content) // 4

            model_info = OPENAI_MODELS.get(model_id, {})
            cost = (
                input_tokens / 1000 * model_info.get("cost_per_1k_input", 0) +
                output_tokens / 1000 * model_info.get("cost_per_1k_output", 0)
            )

        else:  # Gemini
            import google.generativeai as genai
            from llm_engineering.settings import Settings
            from llm_engineering.application.feedback.daily.prompts import get_prompt
            from llm_engineering.application.feedback.document_loader import DocumentLoader

            settings = Settings.load_settings()

            # Gemini API ì„¤ì •
            genai.configure(api_key=settings.GOOGLE_API_KEY)

            # ë¬¸ì„œ ë¡œë“œ
            docs = DocumentLoader.load_with_context(
                target_date=date_str,
                include_previous=True,
                include_next=True,
            )

            # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ê°„ë‹¨í•œ ë²„ì „ - ì‹œê°„ìˆœ ì •ë ¬)
            context_parts = []
            if docs.get("target"):
                context_parts.append(f"## ë¶„ì„ ëŒ€ìƒì¼: {date_str}")

                # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_docs = sorted(
                    docs["target"],
                    key=lambda x: (
                        x.get("ref_date", ""),
                        x.get("metadata", {}).get("start_datetime", "")
                    )
                )

                for doc in sorted_docs:
                    platform = doc.get("platform", "unknown").upper()
                    content = doc.get("content", "")
                    ref_date = doc.get("ref_date", "N/A")
                    context_parts.append(f"### [{platform}] {ref_date}")
                    context_parts.append(content)
                    context_parts.append("")

            context = "\n".join(context_parts)

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = get_prompt(prompt_style)
            full_prompt = f"{system_prompt}\n\n{context}"

            # Gemini ëª¨ë¸ ì´ˆê¸°í™” ë° ìƒì„±
            model = genai.GenerativeModel(model_id)

            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
            )

            start_time = time.time()
            response = model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            feedback_content = response.text
            end_time = time.time()

            generation_time = end_time - start_time

            # ë©”íŠ¸ë¦­ ê³„ì‚° (ëŒ€ëµì )
            input_tokens = len(context) // 4
            output_tokens = len(feedback_content) // 4

            model_info = GEMINI_MODELS.get(model_id, {})
            cost = (
                input_tokens / 1000 * model_info.get("cost_per_1k_input", 0) +
                output_tokens / 1000 * model_info.get("cost_per_1k_output", 0)
            )
            actual_temperature = temperature

        metrics = {
            "generation_time": generation_time,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "estimated_cost": cost,
            "model_id": model_id,
            "provider": provider,
            "temperature": actual_temperature,
            "requested_temperature": temperature,
            "prompt_style": prompt_style,
        }

        return feedback_content, metrics

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return f"âŒ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{error_detail}", {}


def generate_monthly_feedback_with_metrics(
    year: int,
    month: int,
    provider: str,
    model_id: str,
    temperature: float,
    monthly_prompt_style: str = "original",
    df: pd.DataFrame = None,
) -> tuple[str, dict]:
    """
    ì›”ê°„ í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        year: ì—°ë„
        month: ì›”
        provider: ëª¨ë¸ ì œê³µì (OpenAI / Gemini)
        model_id: ëª¨ë¸ ID
        temperature: ì˜¨ë„
        monthly_prompt_style: ì›”ê°„ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ("original", "v2_weekly_based")
        df: ì›”ê°„ ë°ì´í„° DataFrame

    Returns:
        (í”¼ë“œë°± ë‚´ìš©, ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬)
    """
    try:
        if provider == "OpenAI":
            # Temperature ì œí•œ ëª¨ë¸ ì²˜ë¦¬
            temperature_restricted_models = [
                "gpt-5", "gpt-5.1", "gpt-5-pro", "gpt-5-mini", "gpt-5-nano",
                "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"
            ]

            if model_id in temperature_restricted_models:
                actual_temperature = 1.0
            else:
                actual_temperature = temperature

            start_time = time.time()

            if monthly_prompt_style == "v2_weekly_based":
                # V2: ì£¼ê°„ V2 ë¦¬í¬íŠ¸ ê¸°ë°˜ ì›”ê°„ ìš”ì•½
                feedback_content = _generate_monthly_from_weekly_v2(
                    year, month, model_id, actual_temperature, df
                )
            else:
                # Original or Public: ê¸°ì¡´ ê³„ì¸µì  ìš”ì•½ ë°©ì‹
                generator = MonthlyFeedbackGenerator(
                    model_id=model_id,
                    temperature=actual_temperature,
                    prompt_style=monthly_prompt_style if monthly_prompt_style != "v2_weekly_based" else "original",
                )
                feedback_content = generator.generate(
                    year=year,
                    month=month,
                )

            end_time = time.time()
            generation_time = end_time - start_time

            # ë©”íŠ¸ë¦­ ê³„ì‚° (ëŒ€ëµì )
            input_tokens = len(feedback_content) // 4
            output_tokens = len(feedback_content) // 4

            model_info = OPENAI_MODELS.get(model_id, {})
            cost = (
                input_tokens / 1000 * model_info.get("cost_per_1k_input", 0) +
                output_tokens / 1000 * model_info.get("cost_per_1k_output", 0)
            )

        else:  # Gemini
            import google.generativeai as genai
            from llm_engineering.settings import Settings

            settings = Settings.load_settings()

            # Gemini API ì„¤ì •
            genai.configure(api_key=settings.GOOGLE_API_KEY)

            start_time = time.time()

            if monthly_prompt_style == "v2_weekly_based":
                # V2ëŠ” ë³µì¡í•œ ì²´ì¸ì´ë¯€ë¡œ OpenAI ê¶Œì¥
                raise ValueError("V2 ìŠ¤íƒ€ì¼ì€ í˜„ì¬ OpenAI ëª¨ë¸ì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤. OpenAIë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                # Original or Public: MonthlyFeedbackGenerator ì‚¬ìš©
                generator = MonthlyFeedbackGenerator(
                    model_id=model_id,
                    temperature=temperature,
                    prompt_style=monthly_prompt_style if monthly_prompt_style != "v2_weekly_based" else "original",
                )
                feedback_content = generator.generate(
                    year=year,
                    month=month,
                )

            end_time = time.time()
            generation_time = end_time - start_time

            # ë©”íŠ¸ë¦­ ê³„ì‚° (ëŒ€ëµì )
            input_tokens = len(feedback_content) // 4
            output_tokens = len(feedback_content) // 4

            model_info = GEMINI_MODELS.get(model_id, {})
            cost = (
                input_tokens / 1000 * model_info.get("cost_per_1k_input", 0) +
                output_tokens / 1000 * model_info.get("cost_per_1k_output", 0)
            )
            actual_temperature = temperature

        metrics = {
            "generation_time": generation_time,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "estimated_cost": cost,
            "model_id": model_id,
            "provider": provider,
            "temperature": actual_temperature,
            "requested_temperature": temperature,
            "report_type": "monthly",
            "prompt_style": monthly_prompt_style,
        }

        return feedback_content, metrics

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return f"âŒ ì›”ê°„ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{error_detail}", {}


async def _generate_weekly_v2_report_async(
    week_num: int,
    week_start: str,
    week_end: str,
    week_df: pd.DataFrame,
    model_id: str,
    temperature: float,
    prompt_style: str = "v2",
) -> str:
    """
    ë‹¨ì¼ ì£¼ê°„ V2 ë¦¬í¬íŠ¸ë¥¼ ë¹„ë™ê¸°ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        week_num: ì£¼ ë²ˆí˜¸
        week_start: ì£¼ ì‹œì‘ ë‚ ì§œ
        week_end: ì£¼ ì¢…ë£Œ ë‚ ì§œ
        week_df: í•´ë‹¹ ì£¼ì˜ ë°ì´í„° DataFrame
        model_id: ëª¨ë¸ ID
        temperature: ì˜¨ë„
        prompt_style: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ("v2" ë˜ëŠ” "v2_public")

    Returns:
        ì£¼ê°„ V2 ë¦¬í¬íŠ¸ ë¬¸ìì—´
    """
    from llm_engineering.application.feedback.weekly.generator import WeeklyFeedbackGenerator
    from llm_engineering.application.feedback.weekly.metrics import compute_weekly_metrics
    from loguru import logger

    if week_df.empty:
        return f"### Week {week_num}: {week_start} ~ {week_end}\n\në°ì´í„° ì—†ìŒ\n\n---"

    logger.info(f"Generating {prompt_style} report for Week {week_num}: {week_start} ~ {week_end}")

    # ì£¼ê°„ ë©”íŠ¸ë¦­ ê³„ì‚°
    week_metrics = compute_weekly_metrics(week_df, week_start, week_end)

    # ì£¼ê°„ V2 ë¦¬í¬íŠ¸ ìƒì„± (ë¹„ë™ê¸°)
    generator = WeeklyFeedbackGenerator(
        model_id=model_id,
        temperature=temperature,
        prompt_style=prompt_style,
    )

    # generate ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë‚´ë¶€ ë¡œì§ì„ ì§ì ‘ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
    from llm_engineering.application.feedback.document_loader import DocumentLoader
    from langchain_core.messages import HumanMessage

    # ì£¼ê°„ ë°ì´í„° ë¡œë“œ
    weekly_docs = DocumentLoader.load_by_date_range(
        start_date=week_start,
        end_date=week_end,
        sources=["calendar", "notion", "naver_blog"],
        include_weekly_reports=False,
    )

    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context = generator._format_v2_context(
        week_start, week_end, weekly_docs, [], week_metrics
    )

    # LLM ë¹„ë™ê¸° í˜¸ì¶œ
    response = await generator.llm.ainvoke([HumanMessage(content=context)])
    week_feedback = response.content

    # JSON ë¶€ë¶„ ì œê±°
    week_feedback_clean = WeeklyFeedbackGenerator.remove_json_section(week_feedback)

    logger.info(f"Week {week_num} V2 report generated ({len(week_feedback_clean)} chars)")

    return f"### Week {week_num}: {week_start} ~ {week_end}\n\n{week_feedback_clean}\n\n---"


async def _generate_all_weekly_v2_reports_async(
    weeks: list[tuple[str, str]],
    df: pd.DataFrame,
    model_id: str,
    temperature: float,
    prompt_style: str = "v2",
) -> list[str]:
    """
    ëª¨ë“  ì£¼ê°„ V2 ë¦¬í¬íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        weeks: [(week_start, week_end), ...] ë¦¬ìŠ¤íŠ¸
        df: ì›”ê°„ ë°ì´í„° DataFrame
        model_id: ëª¨ë¸ ID
        temperature: ì˜¨ë„
        prompt_style: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ("v2" ë˜ëŠ” "v2_public")

    Returns:
        ì£¼ê°„ V2 ë¦¬í¬íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    import asyncio
    from loguru import logger

    tasks = []
    for week_num, (week_start, week_end) in enumerate(weeks, 1):
        # í•´ë‹¹ ì£¼ì˜ ë°ì´í„° í•„í„°ë§
        week_df = df[
            (df['ref_date'] >= week_start) &
            (df['ref_date'] <= week_end)
        ]

        task = _generate_weekly_v2_report_async(
            week_num, week_start, week_end, week_df, model_id, temperature, prompt_style
        )
        tasks.append(task)

    logger.info(f"Starting parallel generation of {len(tasks)} weekly V2 reports")
    reports = await asyncio.gather(*tasks)
    logger.info(f"Completed parallel generation of {len(reports)} weekly V2 reports")

    return reports


def _generate_monthly_from_weekly_v2(
    year: int,
    month: int,
    model_id: str,
    temperature: float,
    df: pd.DataFrame,
    prompt_style: str = "v2",
) -> str:
    """
    ì£¼ê°„ V2 ë¦¬í¬íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•œ í›„ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›”ê°„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        year: ì—°ë„
        month: ì›”
        model_id: ëª¨ë¸ ID
        temperature: ì˜¨ë„
        df: ì›”ê°„ ë°ì´í„° DataFrame
        prompt_style: í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ("v2" ë˜ëŠ” "v2_public")

    Returns:
        ì›”ê°„ í”¼ë“œë°± ë¬¸ìì—´
    """
    import asyncio
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage

    # 1. ì›”ì„ ì£¼ë³„ë¡œ ë¶„í• 
    start_date = datetime(year, month, 1)
    if month == 12:
        end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
    else:
        end_date = datetime(year, month + 1, 1) - timedelta(days=1)

    weeks = []
    current = start_date
    while current <= end_date:
        week_start = current - timedelta(days=current.weekday())
        week_end = week_start + timedelta(days=6)

        if week_start < start_date:
            week_start = start_date
        if week_end > end_date:
            week_end = end_date

        weeks.append((week_start.strftime("%Y-%m-%d"), week_end.strftime("%Y-%m-%d")))
        current = week_end + timedelta(days=1)

    # ì£¼ê°„ ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼ ê²°ì •
    weekly_style = "v2_public" if prompt_style == "v2_public" else "v2"

    # 2. ê° ì£¼ë³„ V2 ë¦¬í¬íŠ¸ ë³‘ë ¬ ìƒì„±
    weekly_v2_reports = asyncio.run(
        _generate_all_weekly_v2_reports_async(weeks, df, model_id, temperature, weekly_style)
    )

    # 3. ì£¼ê°„ V2 ë¦¬í¬íŠ¸ë“¤ì„ ì¢…í•©í•˜ì—¬ ì›”ê°„ í”¼ë“œë°± ìƒì„±
    if prompt_style == "v2_public":
        # Public ë²„ì „: ê°œì¸ì •ë³´ ë³´í˜¸ ì •ì±… í¬í•¨
        monthly_prompt = f"""ë‹¹ì‹ ì€ **ê³µê°œ ë°°í¬ìš©** ì›”ê°„ í–‰ë™ íŒ¨í„´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**PRIVACY PROTECTION POLICY:**

1. **Personal Information Protection:**
   - âŒ NO people names, places, organization names
   - âŒ NO relationship details beyond time allocation
   - âŒ NO sensitive personal context

2. **Content Disclosure Rules:**
   - âœ… PUBLIC: Work/production, Learning/growth patterns
   - âš ï¸ LIMITED: Recharge, Maintenance - **time only**
   - âŒ PRIVATE: Relationship specifics, personal contexts

3. **Analysis Focus:**
   - Generic behavioral patterns and trends
   - Anonymized triggers and contexts
   - Privacy-safe recommendations

ì•„ë˜ëŠ” {year}ë…„ {month}ì›”ì˜ ì£¼ë³„ V2 ë¦¬í¬íŠ¸ë“¤ì…ë‹ˆë‹¤.
ê° ì£¼ê°„ ë¦¬í¬íŠ¸ëŠ” ì´ë¯¸ ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, íŒ¨í„´ ë¶„ì„ê³¼ ëŒ€í‘œ íƒœê·¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
**ëª¨ë“  ë¦¬í¬íŠ¸ëŠ” ê°œì¸ì •ë³´ ë³´í˜¸ ì •ì±…ì„ ë”°ë¼ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.**

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì´ ì£¼ê°„ ë¦¬í¬íŠ¸ë“¤ì„ ì¢…í•©í•˜ì—¬ **ì›”ê°„ íŠ¸ë Œë“œ**, **ë°˜ë³µ íŒ¨í„´**, **ì¥ê¸°ì  í†µì°°**ì„ ì œê³µí•˜ë˜,
**ê°œì¸ì •ë³´ë¥¼ ì² ì €íˆ ë³´í˜¸**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## ì£¼ê°„ ë¦¬í¬íŠ¸ë“¤

{"".join(weekly_v2_reports)}

## ì¶œë ¥ í˜•ì‹ (Privacy-Protected)

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì›”ê°„ í”¼ë“œë°±ì„ ì‘ì„±í•˜ì„¸ìš”:

## ì›”ê°„ í”¼ë“œë°± ({year}ë…„ {month}ì›”)

[ì´ë²ˆ ë‹¬ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ - ê°œì¸ì •ë³´ ì œì™¸]

---

### ğŸ“Š ì›”ê°„ íŠ¸ë Œë“œ ë¶„ì„

**ì£¼ë³„ ì¶”ì´**:
- Week 1: [ì¼ë°˜í™”ëœ í•µì‹¬ íŒ¨í„´]
- Week 2: [ì¼ë°˜í™”ëœ í•µì‹¬ íŒ¨í„´]
- Week 3: [ì¼ë°˜í™”ëœ í•µì‹¬ íŒ¨í„´]
- Week 4: [ì¼ë°˜í™”ëœ í•µì‹¬ íŒ¨í„´]

**ì „ì²´ íŠ¸ë Œë“œ**: [ê°œì„ /ì•ˆì •/í•˜ë½ ë“±, êµ¬ì²´ì ì´ë˜ ê°œì¸ ë§¥ë½ ì œì™¸]

---

### ğŸ”„ ë°˜ë³µ íŒ¨í„´ ì‹ë³„

**ê¸ì •ì  íŒ¨í„´ (ì§€ì†í•´ì•¼ í•  ê²ƒ)**:
[ì—¬ëŸ¬ ì£¼ì— ê±¸ì³ ë°˜ë³µëœ ì¼ë°˜í™”ëœ ê¸ì •ì  íŒ¨í„´ 2-3ê°œ]

**ë¶€ì •ì  íŒ¨í„´ (ê°œì„ ì´ í•„ìš”í•œ ê²ƒ)**:
[ì—¬ëŸ¬ ì£¼ì— ê±¸ì³ ë°˜ë³µëœ ì¼ë°˜í™”ëœ ë¶€ì •ì  íŒ¨í„´ 2-3ê°œ]

---

### ğŸ¯ ì´ë²ˆ ë‹¬ ì£¼ìš” ì„±ê³¼

[ì›”ê°„ ê´€ì ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì„±ê³¼ 3-4ê°œ, ì¼ë°˜í™”ëœ ì„¤ëª…]

---

### âš ï¸ ì§€ì†ì  ë¬¸ì œ

[ë§¤ì£¼ ë°˜ë³µë˜ê±°ë‚˜ í•´ê²°ë˜ì§€ ì•Šì€ í–‰ë™ ë¬¸ì œë“¤, ê°œì¸ ë§¥ë½ ì œì™¸]

---

### ğŸ’¡ ë‹¤ìŒ ë‹¬ ê°œì„  ì „ëµ

[ì´ë²ˆ ë‹¬ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ êµ¬ì²´ì ì´ë˜ ì¼ë°˜í™”ëœ ì „ëµ 3ê°œ]

---

## PRIVACY RULES

- **Remove ALL personal identifiers** (names, places, organizations)
- **Anonymize all contexts** (use generic terms like "ì§€ì¸", "ëª¨ì„")
- **Time-only for sensitive categories** (relationships, personal activities)
- **Focus on behavioral patterns**, not personal stories
- **Ensure report is safe for public distribution**

**í†¤**: ì¥ê¸°ì  ê´€ì , íŒ¨í„´ ì¤‘ì‹¬, ì „ëµì , ê· í˜•ì , **ê³µê°œ ê°€ëŠ¥**
"""
    else:
        # Original V2 ë²„ì „
        monthly_prompt = f"""ë‹¹ì‹ ì€ ì›”ê°„ í–‰ë™ íŒ¨í„´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” {year}ë…„ {month}ì›”ì˜ ì£¼ë³„ V2 ë¦¬í¬íŠ¸ë“¤ì…ë‹ˆë‹¤.
ê° ì£¼ê°„ ë¦¬í¬íŠ¸ëŠ” ì´ë¯¸ ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, íŒ¨í„´ ë¶„ì„ê³¼ ëŒ€í‘œ íƒœê·¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì´ ì£¼ê°„ ë¦¬í¬íŠ¸ë“¤ì„ ì¢…í•©í•˜ì—¬ **ì›”ê°„ íŠ¸ë Œë“œ**, **ë°˜ë³µ íŒ¨í„´**, **ì¥ê¸°ì  í†µì°°**ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

## ì£¼ê°„ ë¦¬í¬íŠ¸ë“¤

{"".join(weekly_v2_reports)}

## ì¶œë ¥ í˜•ì‹

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì›”ê°„ í”¼ë“œë°±ì„ ì‘ì„±í•˜ì„¸ìš”:

## ì›”ê°„ í”¼ë“œë°± ({year}ë…„ {month}ì›”)

[ì´ë²ˆ ë‹¬ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]

---

### ğŸ“Š ì›”ê°„ íŠ¸ë Œë“œ ë¶„ì„

**ì£¼ë³„ ì¶”ì´**:
- Week 1: [í•µì‹¬ íŒ¨í„´]
- Week 2: [í•µì‹¬ íŒ¨í„´]
- Week 3: [í•µì‹¬ íŒ¨í„´]
- Week 4: [í•µì‹¬ íŒ¨í„´]

**ì „ì²´ íŠ¸ë Œë“œ**: [ê°œì„ /ì•ˆì •/í•˜ë½ ë“±, êµ¬ì²´ì ìœ¼ë¡œ]

---

### ğŸ”„ ë°˜ë³µ íŒ¨í„´ ì‹ë³„

**ê¸ì •ì  íŒ¨í„´ (ì§€ì†í•´ì•¼ í•  ê²ƒ)**:
[ì—¬ëŸ¬ ì£¼ì— ê±¸ì³ ë°˜ë³µëœ ê¸ì •ì  íŒ¨í„´ 2-3ê°œ]

**ë¶€ì •ì  íŒ¨í„´ (ê°œì„ ì´ í•„ìš”í•œ ê²ƒ)**:
[ì—¬ëŸ¬ ì£¼ì— ê±¸ì³ ë°˜ë³µëœ ë¶€ì •ì  íŒ¨í„´ 2-3ê°œ]

---

### ğŸ¯ ì´ë²ˆ ë‹¬ ì£¼ìš” ì„±ê³¼

[ì›”ê°„ ê´€ì ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì„±ê³¼ 3-4ê°œ]

---

### âš ï¸ ì§€ì†ì  ë¬¸ì œ

[ë§¤ì£¼ ë°˜ë³µë˜ê±°ë‚˜ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œë“¤]

---

### ğŸ’¡ ë‹¤ìŒ ë‹¬ ê°œì„  ì „ëµ

[ì´ë²ˆ ë‹¬ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ êµ¬ì²´ì  ì „ëµ 3ê°œ]

---

**í†¤**: ì¥ê¸°ì  ê´€ì , íŒ¨í„´ ì¤‘ì‹¬, ì „ëµì , ê· í˜•ì 
"""

    # LLM í˜¸ì¶œ
    from llm_engineering.settings import Settings
    settings = Settings.load_settings()

    llm = ChatOpenAI(
        model=model_id,
        temperature=temperature,
        openai_api_key=settings.OPENAI_API_KEY
    )
    response = llm.invoke([HumanMessage(content=monthly_prompt)])

    return response.content


def generate_weekly_feedback_with_metrics(
    start_date: str,
    end_date: str,
    provider: str,
    model_id: str,
    temperature: float,
    weekly_prompt_style: str = "original",
    df: pd.DataFrame = None,
) -> tuple[str, dict]:
    """
    ì£¼ê°„ í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        start_date: ì‹œì‘ ë‚ ì§œ
        end_date: ì¢…ë£Œ ë‚ ì§œ
        provider: ëª¨ë¸ ì œê³µì (OpenAI / Gemini)
        model_id: ëª¨ë¸ ID
        temperature: ì˜¨ë„
        weekly_prompt_style: ì£¼ê°„ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ("original", "v2")
        df: ì£¼ê°„ ë°ì´í„° DataFrame (V2 ìŠ¤íƒ€ì¼ì—ì„œ ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)

    Returns:
        (í”¼ë“œë°± ë‚´ìš©, ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬)
    """
    try:
        # V2/V3/V2_PUBLIC ìŠ¤íƒ€ì¼ì¼ ë•Œ ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ ì¤€ë¹„
        precomputed_metrics = None
        if weekly_prompt_style in ["v2", "v3", "v2_public"] and df is not None:
            from llm_engineering.application.feedback.weekly.metrics import compute_weekly_metrics
            precomputed_metrics = compute_weekly_metrics(df, start_date, end_date)

        if provider == "OpenAI":
            # Temperature ì œí•œ ëª¨ë¸ ì²˜ë¦¬
            temperature_restricted_models = [
                "gpt-5", "gpt-5.1", "gpt-5-pro", "gpt-5-mini", "gpt-5-nano",
                "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"
            ]

            if model_id in temperature_restricted_models:
                generator = WeeklyFeedbackGenerator(
                    model_id=model_id,
                    temperature=1.0,
                    prompt_style=weekly_prompt_style,
                )
                actual_temperature = 1.0
            else:
                generator = WeeklyFeedbackGenerator(
                    model_id=model_id,
                    temperature=temperature,
                    prompt_style=weekly_prompt_style,
                )
                actual_temperature = temperature

            start_time = time.time()

            feedback_content = generator.generate(
                start_date=start_date,
                end_date=end_date,
                include_past_reports=True,
                past_reports_limit=4,
                precomputed_metrics=precomputed_metrics,
                save_to_db=False  # ì‹¤í—˜ìš©ì´ë¯€ë¡œ DBì— ì €ì¥ ì•ˆ í•¨
            )

            end_time = time.time()
            generation_time = end_time - start_time

            # ë©”íŠ¸ë¦­ ê³„ì‚° (ëŒ€ëµì )
            input_tokens = len(feedback_content) // 4
            output_tokens = len(feedback_content) // 4

            model_info = OPENAI_MODELS.get(model_id, {})
            cost = (
                input_tokens / 1000 * model_info.get("cost_per_1k_input", 0) +
                output_tokens / 1000 * model_info.get("cost_per_1k_output", 0)
            )

        else:  # Gemini
            import google.generativeai as genai
            from llm_engineering.settings import Settings
            from llm_engineering.application.feedback.weekly.prompts import WEEKLY_FEEDBACK_PROMPT, get_weekly_prompt
            from llm_engineering.application.feedback.document_loader import DocumentLoader
            import json

            settings = Settings.load_settings()

            # Gemini API ì„¤ì •
            genai.configure(api_key=settings.GOOGLE_API_KEY)

            # ì£¼ê°„ ë¬¸ì„œ ë¡œë“œ
            weekly_docs = DocumentLoader.load_by_date_range(
                start_date=start_date,
                end_date=end_date,
                sources=["calendar", "notion", "naver_blog"],
                include_weekly_reports=False,
            )

            # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            context_parts = []
            context_parts.append(f"## ì£¼ê°„ ë¶„ì„ ëŒ€ìƒ: {start_date} ~ {end_date}")
            context_parts.append("")

            if weekly_docs:
                # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
                docs_by_date = {}
                for doc in weekly_docs:
                    ref_date = doc.get("ref_date", "unknown")
                    if ref_date not in docs_by_date:
                        docs_by_date[ref_date] = []
                    docs_by_date[ref_date].append(doc)

                for date in sorted(docs_by_date.keys()):
                    day_docs = docs_by_date[date]
                    context_parts.append(f"### {date}")
                    for doc in day_docs:
                        platform = doc.get("platform", "unknown").upper()
                        content = doc.get("content", "")
                        context_parts.append(f"[{platform}] {content}")
                    context_parts.append("")

            context = "\n".join(context_parts)

            # í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ì²˜ë¦¬
            if weekly_prompt_style == "v3":
                # V3ëŠ” 2ë‹¨ê³„ ì²´ì¸ì´ë¯€ë¡œ Geminiì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ
                raise ValueError("V3 ìŠ¤íƒ€ì¼ì€ í˜„ì¬ OpenAI ëª¨ë¸ì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤. OpenAIë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            elif weekly_prompt_style == "v2" and precomputed_metrics:
                # V2: ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ í¬í•¨
                prompt_template = get_weekly_prompt("v2")
                full_prompt = prompt_template.format(
                    start_date=start_date,
                    end_date=end_date,
                    precomputed_metrics=json.dumps(precomputed_metrics, ensure_ascii=False, indent=2),
                    raw_logs=context,
                    previous_week_summary="ì—†ìŒ",
                )
            else:
                # Original: ê¸°ì¡´ ë°©ì‹
                full_prompt = f"{WEEKLY_FEEDBACK_PROMPT}\n\n{context}"

            # Gemini ëª¨ë¸ ì´ˆê¸°í™” ë° ìƒì„±
            model = genai.GenerativeModel(model_id)

            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
            )

            start_time = time.time()
            response = model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            feedback_content = response.text
            end_time = time.time()

            generation_time = end_time - start_time

            # ë©”íŠ¸ë¦­ ê³„ì‚° (ëŒ€ëµì )
            input_tokens = len(context) // 4
            output_tokens = len(feedback_content) // 4

            model_info = GEMINI_MODELS.get(model_id, {})
            cost = (
                input_tokens / 1000 * model_info.get("cost_per_1k_input", 0) +
                output_tokens / 1000 * model_info.get("cost_per_1k_output", 0)
            )
            actual_temperature = temperature

        metrics = {
            "generation_time": generation_time,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "estimated_cost": cost,
            "model_id": model_id,
            "provider": provider,
            "temperature": actual_temperature,
            "requested_temperature": temperature,
            "report_type": "weekly",
            "prompt_style": weekly_prompt_style,
        }

        return feedback_content, metrics

    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        return f"âŒ ì£¼ê°„ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{error_detail}", {}


def show_llm_feedback_experiment(date_str: str, provider: str, model_id: str, temperature: float, prompt_style: str):
    """ì‹¤í—˜ìš© LLM í”¼ë“œë°± ì˜ì—­ (ì¼ë³„)"""

    # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if provider == "OpenAI":
        model_info = OPENAI_MODELS.get(model_id, {})
    else:
        model_info = GEMINI_MODELS.get(model_id, {})

    model_name = model_info.get("name", model_id)

    st.caption(f"ğŸ§ª ì‹¤í—˜ ëª¨ë“œ - {provider}: {model_name}, Temperature: {temperature}, ìŠ¤íƒ€ì¼: {prompt_style}")

    # í”¼ë“œë°± ìƒì„± ë²„íŠ¼
    if st.button("ğŸš€ í”¼ë“œë°± ìƒì„±", type="primary", use_container_width=True):
        with st.spinner("í”¼ë“œë°± ìƒì„± ì¤‘..."):
            feedback, metrics = generate_feedback_with_metrics(
                date_str, provider, model_id, temperature, prompt_style
            )

            if metrics:
                # ë©”íŠ¸ë¦­ í‘œì‹œ
                st.success("âœ… í”¼ë“œë°±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                # Temperature ì¡°ì • ê²½ê³ 
                if metrics.get("temperature") != metrics.get("requested_temperature"):
                    st.warning(f"âš ï¸ ì´ ëª¨ë¸ì€ temperature ì¡°ì •ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {metrics['temperature']}ì´(ê°€) ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ìƒì„± ì‹œê°„", f"{metrics['generation_time']:.2f}s")
                with col2:
                    st.metric("ì´ í† í°", f"{metrics['total_tokens']:,}")
                with col3:
                    st.metric("ì˜ˆìƒ ë¹„ìš©", f"${metrics['estimated_cost']:.4f}")
                with col4:
                    st.metric("Temperature", f"{metrics['temperature']}")

            st.markdown("---")
            st.markdown("### ğŸ“‹ ì¼ì¼ í”¼ë“œë°±")
            st.markdown(feedback)
    else:
        st.info(f"""
        **ğŸ§ª ì‹¤í—˜ìš© ëŒ€ì‹œë³´ë“œ - {provider}**

        ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
        - ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ Provider ì„ íƒ (OpenAI / Gemini)
        - ëª¨ë¸ ì„ íƒ ({len(OPENAI_MODELS if provider == 'OpenAI' else GEMINI_MODELS)}ê°œ ëª¨ë¸)
        - Temperature ì¡°ì • (0.0~1.0)
        - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ë³€ê²½
        - í”„ë¼ì´ë²„ì‹œ í•„í„° on/off

        ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ í”¼ë“œë°±ì„ ìƒì„±í•˜ì„¸ìš”.
        """)


def show_monthly_llm_feedback_experiment(
    year: int,
    month: int,
    provider: str,
    model_id: str,
    temperature: float,
    monthly_prompt_style: str = "original",
    df: pd.DataFrame = None
):
    """ì‹¤í—˜ìš© LLM í”¼ë“œë°± ì˜ì—­ (ì›”ê°„)"""

    # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if provider == "OpenAI":
        model_info = OPENAI_MODELS.get(model_id, {})
    else:
        model_info = GEMINI_MODELS.get(model_id, {})

    model_name = model_info.get("name", model_id)

    style_labels = {
        "original": "Original (ê³„ì¸µì )",
        "v2_weekly_based": "V2 (ì£¼ê°„ V2 ê¸°ë°˜)"
    }
    style_label = style_labels.get(monthly_prompt_style, "Original")
    st.caption(f"ğŸ§ª ì›”ê°„ ì‹¤í—˜ ëª¨ë“œ - {provider}: {model_name}, Temperature: {temperature}, ìŠ¤íƒ€ì¼: {style_label}")

    # í”¼ë“œë°± ìƒì„± ë²„íŠ¼
    if st.button("ğŸš€ ì›”ê°„ í”¼ë“œë°± ìƒì„±", type="primary", use_container_width=True):
        with st.spinner(f"ì›”ê°„ í”¼ë“œë°± ìƒì„± ì¤‘... ({year}ë…„ {month}ì›” ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)"):
            feedback, metrics = generate_monthly_feedback_with_metrics(
                year, month, provider, model_id, temperature,
                monthly_prompt_style=monthly_prompt_style,
                df=df
            )

            if metrics:
                # ë©”íŠ¸ë¦­ í‘œì‹œ
                st.success("âœ… ì›”ê°„ í”¼ë“œë°±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                # Temperature ì¡°ì • ê²½ê³ 
                if metrics.get("temperature") != metrics.get("requested_temperature"):
                    st.warning(f"âš ï¸ ì´ ëª¨ë¸ì€ temperature ì¡°ì •ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {metrics['temperature']}ì´(ê°€) ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ìƒì„± ì‹œê°„", f"{metrics['generation_time']:.2f}s")
                with col2:
                    st.metric("ì´ í† í°", f"{metrics['total_tokens']:,}")
                with col3:
                    st.metric("ì˜ˆìƒ ë¹„ìš©", f"${metrics['estimated_cost']:.4f}")
                with col4:
                    st.metric("í”„ë¡¬í”„íŠ¸", metrics.get('prompt_style', 'original'))

            st.markdown("---")
            st.markdown(f"### ğŸ“‹ ì›”ê°„ í”¼ë“œë°± ({year}ë…„ {month}ì›”)")
            st.markdown(feedback)
    else:
        st.info(f"""
        **ğŸ§ª ì›”ê°„ ì‹¤í—˜ìš© ëŒ€ì‹œë³´ë“œ - {provider}**

        í•œ ë‹¬ê°„ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì›”ê°„ í”¼ë“œë°±ì„ ìƒì„±í•©ë‹ˆë‹¤:
        - ê¸°ê°„: {year}ë…„ {month}ì›”
        - ëª¨ë¸: {model_name}
        - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {style_label}

        **Original ìŠ¤íƒ€ì¼:**
        - ì£¼ë³„ ìš”ì•½ í›„ ì›”ê°„ í”¼ë“œë°± ìƒì„± (ê³„ì¸µì  ìš”ì•½)

        **V2 ìŠ¤íƒ€ì¼ (ì‹¤í—˜):**
        - ê° ì£¼ë³„ë¡œ V2 ë¦¬í¬íŠ¸ ìƒì„± (ì‚¬ì „ê³„ì‚° ë©”íŠ¸ë¦­ + íŒ¨í„´ ë¶„ì„)
        - ì£¼ê°„ V2 ë¦¬í¬íŠ¸ë“¤ì„ ì¢…í•©í•˜ì—¬ ì›”ê°„ íŠ¸ë Œë“œ ë¶„ì„
        - ë” ì •í™•í•˜ì§€ë§Œ ì‹œê°„ê³¼ ë¹„ìš©ì´ ë§ì´ ë“¦

        ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì›”ê°„ í”¼ë“œë°±ì„ ìƒì„±í•˜ì„¸ìš”.
        """)


def show_weekly_llm_feedback_experiment(
    start_date: str,
    end_date: str,
    provider: str,
    model_id: str,
    temperature: float,
    weekly_prompt_style: str = "original",
    df: pd.DataFrame = None
):
    """ì‹¤í—˜ìš© LLM í”¼ë“œë°± ì˜ì—­ (ì£¼ê°„)"""

    # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    if provider == "OpenAI":
        model_info = OPENAI_MODELS.get(model_id, {})
    else:
        model_info = GEMINI_MODELS.get(model_id, {})

    model_name = model_info.get("name", model_id)

    style_labels = {
        "original": "Original",
        "v2": "V2 (ì‚¬ì „ê³„ì‚°)",
        "v3": "V3 (2ë‹¨ê³„ ì²´ì¸)"
    }
    style_label = style_labels.get(weekly_prompt_style, "Original")
    st.caption(f"ğŸ§ª ì£¼ê°„ ì‹¤í—˜ ëª¨ë“œ - {provider}: {model_name}, Temperature: {temperature}, ìŠ¤íƒ€ì¼: {style_label}")

    # V2/V3/V2_PUBLIC ìŠ¤íƒ€ì¼ì¼ ë•Œ ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ ë¯¸ë¦¬ë³´ê¸°
    if weekly_prompt_style in ["v2", "v3", "v2_public"] and df is not None:
        with st.expander("ğŸ“Š ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ ë¯¸ë¦¬ë³´ê¸°"):
            from llm_engineering.application.feedback.weekly.metrics import compute_weekly_metrics
            precomputed = compute_weekly_metrics(df, start_date, end_date)

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì‹œê°„", f"{precomputed['summary']['total_hours']}h")
                st.metric("í™œë™ ìˆ˜", precomputed['summary']['total_activities'])
            with col2:
                st.metric("í‰ê·  ìˆ˜ë©´", f"{precomputed['sleep']['avg_h']}h")
                recovery = precomputed.get('recovery_ratio')
                st.metric("íšŒë³µ ë¹„ìœ¨", f"{recovery}" if recovery else "N/A (Drain ì—†ìŒ)")
            with col3:
                st.metric("Drain%", f"{precomputed['drain']['drain_percent']}%")
                st.metric("ì‹¬ì•¼ Drain", f"{precomputed['drain']['late_night_minutes_23_03']}ë¶„")

    # í”¼ë“œë°± ìƒì„± ë²„íŠ¼
    if st.button("ğŸš€ ì£¼ê°„ í”¼ë“œë°± ìƒì„±", type="primary", use_container_width=True):
        with st.spinner("ì£¼ê°„ í”¼ë“œë°± ìƒì„± ì¤‘... (7ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤)"):
            feedback, metrics = generate_weekly_feedback_with_metrics(
                start_date, end_date, provider, model_id, temperature,
                weekly_prompt_style=weekly_prompt_style,
                df=df
            )

            if metrics:
                # ë©”íŠ¸ë¦­ í‘œì‹œ
                st.success("âœ… ì£¼ê°„ í”¼ë“œë°±ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                # Temperature ì¡°ì • ê²½ê³ 
                if metrics.get("temperature") != metrics.get("requested_temperature"):
                    st.warning(f"âš ï¸ ì´ ëª¨ë¸ì€ temperature ì¡°ì •ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ {metrics['temperature']}ì´(ê°€) ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ìƒì„± ì‹œê°„", f"{metrics['generation_time']:.2f}s")
                with col2:
                    st.metric("ì´ í† í°", f"{metrics['total_tokens']:,}")
                with col3:
                    st.metric("ì˜ˆìƒ ë¹„ìš©", f"${metrics['estimated_cost']:.4f}")
                with col4:
                    st.metric("í”„ë¡¬í”„íŠ¸", metrics.get('prompt_style', 'original'))

            st.markdown("---")
            st.markdown("### ğŸ“‹ ì£¼ê°„ í”¼ë“œë°±")

            # V2/V3/public ìŠ¤íƒ€ì¼ì€ JSON ë¶€ë¶„ ì œê±°í•˜ê³  ë¦¬í¬íŠ¸ë§Œ í‘œì‹œ
            if weekly_prompt_style in ["v2", "v3", "public", "v2_public"]:
                from llm_engineering.application.feedback.weekly.generator import WeeklyFeedbackGenerator
                display_feedback = WeeklyFeedbackGenerator.remove_json_section(feedback)
                st.markdown(display_feedback)
            else:
                st.markdown(feedback)
    else:
        st.info(f"""
        **ğŸ§ª ì£¼ê°„ ì‹¤í—˜ìš© ëŒ€ì‹œë³´ë“œ - {provider}**

        7ì¼ê°„ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ê°„ í”¼ë“œë°±ì„ ìƒì„±í•©ë‹ˆë‹¤:
        - ê¸°ê°„: {start_date} ~ {end_date}
        - ëª¨ë¸: {model_name}
        - í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼: {style_label}
        - ê³¼ê±° ì£¼ê°„ ë¦¬í¬íŠ¸ ì°¸ì¡° (ìµœëŒ€ 4ê°œ)

        **V2 ìŠ¤íƒ€ì¼ íŠ¹ì§•:**
        - ì‚¬ì „ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ ì‚¬ìš© (LLM ì—°ì‚° ê°ì†Œ)
        - íŒ¨í„´ ë¶„ì„ì— ì§‘ì¤‘
        - ì´ë²ˆ ì£¼ ëŒ€í‘œ íƒœê·¸ ì¶”ì¶œ

        ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì£¼ê°„ í”¼ë“œë°±ì„ ìƒì„±í•˜ì„¸ìš”.
        """)


def main():
    st.title("ğŸ§ª í™œë™ ë¦¬í¬íŠ¸ (ì‹¤í—˜ìš©)")
    st.markdown("---")

    # ì‚¬ì´ë“œë°” ì„¤ì • (ëª¨ë“  íƒ­ ê³µí†µ)
    with st.sidebar:
        st.header("âš™ï¸ ì‹¤í—˜ ì„¤ì •")

        st.markdown("---")

        # Provider ì„ íƒ
        st.subheader("ğŸŒ Provider ì„ íƒ")
        provider = st.radio(
            "LLM Provider",
            options=["OpenAI", "Gemini"],
            index=0,
            help="ëª¨ë¸ ì œê³µìë¥¼ ì„ íƒí•˜ì„¸ìš”"
        )

        # ì„ íƒí•œ Providerì— ë”°ë¼ ëª¨ë¸ ëª©ë¡ ë³€ê²½
        if provider == "OpenAI":
            available_models = OPENAI_MODELS
            default_index = list(OPENAI_MODELS.keys()).index("gpt-4o-mini") if "gpt-4o-mini" in OPENAI_MODELS else 0
        else:
            available_models = GEMINI_MODELS
            default_index = list(GEMINI_MODELS.keys()).index("gemini-2.5-flash") if "gemini-2.5-flash" in GEMINI_MODELS else 0

        st.markdown("---")

        # ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– ëª¨ë¸ ì„ íƒ")
        model_id = st.selectbox(
            "LLM ëª¨ë¸",
            options=list(available_models.keys()),
            format_func=lambda x: available_models[x]["name"],
            index=default_index
        )

        # Temperature ì„¤ì •
        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            help="ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì´ì§€ë§Œ ì¼ê´€ì„±ì´ ë–¨ì–´ì§"
        )

        # Temperature ì œí•œ ëª¨ë¸ ê²½ê³ 
        if provider == "OpenAI":
            temperature_restricted = [
                "gpt-5", "gpt-5.1", "gpt-5-pro", "gpt-5-mini", "gpt-5-nano",
                "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"
            ]
            if model_id in temperature_restricted:
                st.caption("âš ï¸ ì´ ëª¨ë¸ì€ temperature ì¡°ì •ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(1.0)ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.")

        st.markdown("---")

        # í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼ ì„ íƒ
        st.subheader("ğŸ“ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼")

        # ì¼ë³„
        prompt_style = st.selectbox(
            "ì¼ë³„",
            options=PROMPT_STYLES,
            index=0
        )

        # ì£¼ê°„
        weekly_prompt_style = st.selectbox(
            "ì£¼ê°„",
            options=["original", "v2", "v3", "public", "v2_public"],
            format_func=lambda x: {
                "original": "Original",
                "v2": "V2 (ì‚¬ì „ê³„ì‚°)",
                "v3": "V3 (2ë‹¨ê³„ ì²´ì¸)",
                "public": "Public (ê°œì¸ì •ë³´ ë³´í˜¸)",
                "v2_public": "V2 Public (ì‚¬ì „ê³„ì‚° + ê°œì¸ì •ë³´ ë³´í˜¸)"
            }[x],
            index=0
        )

        # ì›”ê°„
        monthly_prompt_style = st.selectbox(
            "ì›”ê°„",
            options=["original", "v2_weekly_based", "public", "v2_public"],
            format_func=lambda x: {
                "original": "Original (ê³„ì¸µì )",
                "v2_weekly_based": "V2 (ì£¼ê°„ V2 ê¸°ë°˜)",
                "public": "Public (ê°œì¸ì •ë³´ ë³´í˜¸)",
                "v2_public": "V2 Public (ì£¼ê°„ V2 ê¸°ë°˜ + ê°œì¸ì •ë³´ ë³´í˜¸)"
            }[x],
            index=0
        )

        st.markdown("---")

        # í”„ë¼ì´ë²„ì‹œ í•„í„° í† ê¸€
        st.subheader("ğŸ”’ í”„ë¼ì´ë²„ì‹œ")
        apply_privacy = st.toggle(
            "í•„í„° ì ìš©",
            value=False,
            help="ì¼œë©´ ì¸ê°„ê´€ê³„ ê´€ë ¨ ë©”ëª¨ê°€ ë§ˆìŠ¤í‚¹ë©ë‹ˆë‹¤"
        )

    # ë©”ì¸ ì˜ì—­: íƒ­ìœ¼ë¡œ ë¶„ë¦¬
    tab_daily, tab_weekly, tab_monthly = st.tabs(["ğŸ“… ì¼ë³„ ë¦¬í¬íŠ¸", "ğŸ“Š ì£¼ê°„ ë¦¬í¬íŠ¸", "ğŸ“ˆ ì›”ê°„ ë¦¬í¬íŠ¸"])

    # ===== ì¼ë³„ ë¦¬í¬íŠ¸ íƒ­ =====
    with tab_daily:
        # ë‚ ì§œ ì„ íƒ
        col_date, col_btn = st.columns([3, 1])
        with col_date:
            selected_date = st.date_input(
                "ë¶„ì„í•  ë‚ ì§œ",
                value=datetime.now().date(),
                key="daily_date"
            )
        with col_btn:
            st.write("")  # ê°„ê²© ì¡°ì •
            if st.button("ğŸ“¥ ë¡œë“œ", type="primary", key="daily_load", use_container_width=True):
                st.cache_data.clear()
                st.rerun()

        date_str = selected_date.strftime("%Y-%m-%d")

        # ë°ì´í„° ë¡œë“œ
        with st.spinner("ë°ì´í„° ë¡œë”© ì¤‘..."):
            df = load_daily_data(date_str, apply_privacy_filter=apply_privacy)

        if df is None:
            st.error(f"âš ï¸  {date_str}ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ë‹¤ë¥¸ ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        weekday = get_weekday_korean(date_str)
        st.success(f"âœ… {date_str} ({weekday}) ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì´ {len(df)}ê°œ í™œë™)")

        # 2-Column Layout
        left_col, right_col = st.columns([2, 1])

        with left_col:
            show_statistics(df, date_str)
            st.markdown("---")
            show_agency_pie_chart(df)
            st.markdown("---")
            show_category_distribution(df)
            st.markdown("---")
            show_sleep_breakdown(df)
            st.markdown("---")
            show_five_areas_analysis(df)

        with right_col:
            st.header("ğŸ§ª LLM í”¼ë“œë°±")
            show_llm_feedback_experiment(date_str, provider, model_id, temperature, prompt_style)

    # ===== ì£¼ê°„ ë¦¬í¬íŠ¸ íƒ­ =====
    with tab_weekly:
        # ë‚ ì§œ ì„ íƒ
        today = datetime.now().date()
        days_since_monday = today.weekday()
        last_monday = today - timedelta(days=days_since_monday + 7)
        last_sunday = last_monday + timedelta(days=6)

        col_start, col_end, col_btn = st.columns([2, 2, 1])
        with col_start:
            start_date = st.date_input(
                "ì‹œì‘ì¼ (ì›”ìš”ì¼)",
                value=last_monday,
                key="weekly_start"
            )
        with col_end:
            end_date = st.date_input(
                "ì¢…ë£Œì¼ (ì¼ìš”ì¼)",
                value=last_sunday,
                key="weekly_end"
            )
        with col_btn:
            st.write("")  # ê°„ê²© ì¡°ì •
            if st.button("ğŸ“¥ ë¡œë“œ", type="primary", key="weekly_load", use_container_width=True):
                st.cache_data.clear()
                st.rerun()

        start_date_str = start_date.strftime("%Y-%m-%d")
        end_date_str = end_date.strftime("%Y-%m-%d")

        # ë°ì´í„° ë¡œë“œ
        with st.spinner(f"ì£¼ê°„ ë°ì´í„° ë¡œë”© ì¤‘... ({start_date_str} ~ {end_date_str})"):
            df = load_weekly_data(start_date_str, end_date_str, apply_privacy_filter=apply_privacy)

        if df is None:
            st.error(f"âš ï¸  {start_date_str} ~ {end_date_str} ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ë‹¤ë¥¸ ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        st.success(f"âœ… ì£¼ê°„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {start_date_str} ~ {end_date_str} (ì´ {len(df)}ê°œ í™œë™)")

        # 2-Column Layout: ì™¼ìª½(ì‹œê°í™”/í†µê³„), ì˜¤ë¥¸ìª½(LLM í”¼ë“œë°±)
        left_col, right_col = st.columns([2, 1])

        with left_col:
            # 1. ì£¼ê°„ í†µê³„
            show_statistics(df, start_date_str, is_weekly=True, start_date=start_date_str, end_date=end_date_str)

            st.markdown("---")

            # 2. Agency íŒŒì´ì°¨íŠ¸
            show_agency_pie_chart(df)

            st.markdown("---")

            # 3. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
            show_category_distribution(df)

            st.markdown("---")

            # 4. ì¼ë³„ ìš”ì•½ (ì£¼ê°„ ì „ìš©)
            st.subheader("ğŸ“… ì¼ë³„ ìš”ì•½")
            if 'ref_date' in df.columns:
                daily_summary = df.groupby('ref_date').agg({
                    'duration_minutes': 'sum',
                    'original_id': 'count',
                    'has_relationship_tag': 'sum',
                    'is_risky_recharger': 'sum'
                }).rename(columns={
                    'duration_minutes': 'ì´ ì‹œê°„(ë¶„)',
                    'original_id': 'í™œë™ ìˆ˜',
                    'has_relationship_tag': '#ì¸ê°„ê´€ê³„',
                    'is_risky_recharger': '#ì¦‰ì‹œë§Œì¡±'
                })
                daily_summary['ì´ ì‹œê°„'] = daily_summary['ì´ ì‹œê°„(ë¶„)'].apply(format_duration)
                st.dataframe(
                    daily_summary[['ì´ ì‹œê°„', 'í™œë™ ìˆ˜', '#ì¸ê°„ê´€ê³„', '#ì¦‰ì‹œë§Œì¡±']],
                    use_container_width=True
                )
            else:
                st.info("ì¼ë³„ ìš”ì•½ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            st.markdown("---")
            st.info("ğŸ’¡ ì£¼ê°„ ì‹œê°í™”ëŠ” ì¶”í›„ ì—…ë°ì´íŠ¸ ì˜ˆì •ì…ë‹ˆë‹¤. í˜„ì¬ëŠ” ê¸°ë³¸ í†µê³„ë§Œ ì œê³µë©ë‹ˆë‹¤.")

        with right_col:
            st.header("ğŸ§ª LLM í”¼ë“œë°±")
            show_weekly_llm_feedback_experiment(
                start_date_str, end_date_str, provider, model_id, temperature,
                weekly_prompt_style=weekly_prompt_style,
                df=df
            )

    # ===== ì›”ê°„ ë¦¬í¬íŠ¸ íƒ­ =====
    with tab_monthly:
        # ë‚ ì§œ ì„ íƒ
        today = datetime.now().date()

        col_year, col_month, col_btn = st.columns([2, 2, 1])
        with col_year:
            year = st.number_input(
                "ì—°ë„",
                min_value=2020,
                max_value=2030,
                value=today.year,
                step=1,
                key="monthly_year"
            )
        with col_month:
            month = st.number_input(
                "ì›”",
                min_value=1,
                max_value=12,
                value=today.month,
                step=1,
                key="monthly_month"
            )
        with col_btn:
            st.write("")  # ê°„ê²© ì¡°ì •
            if st.button("ğŸ“¥ ë¡œë“œ", type="primary", key="monthly_load", use_container_width=True):
                st.cache_data.clear()
                st.rerun()

        # ë°ì´í„° ë¡œë“œ
        with st.spinner(f"ì›”ê°„ ë°ì´í„° ë¡œë”© ì¤‘... ({year}ë…„ {month}ì›”)"):
            df = load_monthly_data(year, month, apply_privacy_filter=apply_privacy)

        if df is None:
            st.error(f"âš ï¸  {year}ë…„ {month}ì›”ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ë‹¤ë¥¸ ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        st.success(f"âœ… ì›”ê°„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {year}ë…„ {month}ì›” (ì´ {len(df)}ê°œ í™œë™)")

        # 2-Column Layout
        left_col, right_col = st.columns([2, 1])

        with left_col:
            st.subheader(f"ğŸ“Š {year}ë…„ {month}ì›” ì „ì²´ í†µê³„")

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ê¸°ë¡ ì‹œê°„", format_duration(df['duration_minutes'].sum()))
            with col2:
                st.metric("ì´ í™œë™ ìˆ˜", f"{len(df)}ê°œ")
            with col3:
                st.metric("#ì¸ê°„ê´€ê³„", f"{df['has_relationship_tag'].sum()}ê°œ")
            with col4:
                st.metric("#ì¦‰ì‹œë§Œì¡±", f"{df['is_risky_recharger'].sum()}ê°œ")

            st.markdown("---")
            show_agency_pie_chart(df)
            st.markdown("---")
            show_category_distribution(df)
            st.markdown("---")

            st.subheader("ğŸ“… ì£¼ë³„ ìš”ì•½")
            if 'ref_date' in df.columns:
                df_copy = df.copy()
                df_copy['week'] = pd.to_datetime(df_copy['ref_date']).dt.isocalendar().week
                weekly_summary = df_copy.groupby('week').agg({
                    'duration_minutes': 'sum',
                    'original_id': 'count',
                    'has_relationship_tag': 'sum',
                    'is_risky_recharger': 'sum'
                }).rename(columns={
                    'duration_minutes': 'ì´ ì‹œê°„(ë¶„)',
                    'original_id': 'í™œë™ ìˆ˜',
                    'has_relationship_tag': '#ì¸ê°„ê´€ê³„',
                    'is_risky_recharger': '#ì¦‰ì‹œë§Œì¡±'
                })
                weekly_summary['ì´ ì‹œê°„'] = weekly_summary['ì´ ì‹œê°„(ë¶„)'].apply(format_duration)
                st.dataframe(
                    weekly_summary[['ì´ ì‹œê°„', 'í™œë™ ìˆ˜', '#ì¸ê°„ê´€ê³„', '#ì¦‰ì‹œë§Œì¡±']],
                    use_container_width=True
                )
            else:
                st.info("ì£¼ë³„ ìš”ì•½ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with right_col:
            st.header("ğŸ§ª LLM í”¼ë“œë°±")
            show_monthly_llm_feedback_experiment(
                year, month, provider, model_id, temperature,
                monthly_prompt_style=monthly_prompt_style,
                df=df
            )


if __name__ == "__main__":
    main()
