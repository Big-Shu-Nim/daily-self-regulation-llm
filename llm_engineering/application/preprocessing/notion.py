"""
Notion data preprocessor.

Notion í˜ì´ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë¬¸ì„œ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
"""

import re
from typing import List, Dict, Any, Tuple, Optional

import pandas as pd

from .base import BasePreprocessor
from .utils import (
    get_ancestor_chain,
    filter_by_ancestor_title,
    filter_by_parent_level_and_title,
    extract_date_from_text,
    extract_ref_date_from_title,
    extract_week_range_from_title,
    NOTION_DATE_PATTERNS,
    clean_text
)


class NotionPreprocessor(BasePreprocessor):
    """
    Notion ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.

    í•µì‹¬ ì‘ì—…:
    1. ë¬´íš¨ ë¬¸ì„œ ë§ˆí‚¹ (ë¹ˆ ë‚´ìš©, untitled, í…œí”Œë¦¿ë§Œ ìˆëŠ” ê²½ìš°)
    2. Ancestor chain ìƒì„±
    3. ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜:
       - daily_log_company: íšŒì‚¬ ì¼ì¼ì—…ë¬´ì •ë¦¬
       - diary: ìŠµê´€ íŠ¸ë˜ì»¤ ì¼ê¸°
       - weekly_report: ì£¼ê°„ì—…ë¬´ì •ë¦¬
       - general: ê¸°íƒ€
    4. ref_date ì¶”ì¶œ (title ë˜ëŠ” ancestor chainì—ì„œ)
    5. ìì—°ì–´ content ìƒì„± (ì œëª© + ê²½ë¡œ + ë³¸ë¬¸)
    """

    # í…œí”Œë¦¿ íŒ¨í„´
    TEMPLATE_PATTERN = re.compile(
        r"^(?:###\s*(ì˜¤ëŠ˜ì˜\s*íŠ¹ë³„í•œ\s*ì |ì˜¤ëŠ˜ì˜\s*í•˜ì´ë¼ì´íŠ¸|ì…€í”„\s*íšŒê³ \s*:\s*ì¹­ì°¬|"
        r"ì…€í”„\s*íšŒê³ \s*:\s*ë°˜ì„±|ë‚´ì¼\s*ê¸°ëŒ€ë˜ëŠ”\s*ì²«ì‘ì—…)\s*-\s*\n*)+$",
        flags=re.MULTILINE
    )

    # ìŠµê´€ íŠ¸ë˜ì»¤ ì „ìš© í…œí”Œë¦¿ íŒ¨í„´
    HABIT_TEMPLATE_PATTERN = re.compile(
        r"^(?:\s*"
        r"(?:###\s*(ì˜¤ëŠ˜ì˜\s*íŠ¹ë³„í•œ\s*ì |ì˜¤ëŠ˜ì˜\s*í•˜ì´ë¼ì´íŠ¸|ì…€í”„\s*íšŒê³ \s*:\s*ì¹­ì°¬|"
        r"ì…€í”„\s*íšŒê³ \s*:\s*ë°˜ì„±|ë‚´ì¼\s*ê¸°ëŒ€ë˜ëŠ”\s*ì²«ì‘ì—…)\s*-\s*)"
        r"[\n\s]*)+$",
        flags=re.MULTILINE
    )

    def clean(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        Notion DataFrameì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            df: ì›ë³¸ Notion DataFrame

        Returns:
            CleanedNotionDocumentì— ë§ëŠ” dict ë¦¬ìŠ¤íŠ¸
        """
        self.log("="*50)
        self.log(f"Notion ì „ì²˜ë¦¬ ì‹œì‘: {len(df)}ê±´")

        # 1. í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
        required_columns = [
            'id', 'title', 'content', 'ancestors', 'notion_page_id',
            'url', 'created_time', 'last_edited_time', 'author_id', 'author_full_name'
        ]
        self._validate_dataframe(df, required_columns)

        # 2. ë¬´íš¨ ë¬¸ì„œ ë§ˆí‚¹
        df = self._mark_invalid_documents(df)

        # 3. Ancestor chain ìƒì„±
        df["ancestor_chain"] = df["ancestors"].apply(get_ancestor_chain)
        self.log("âœ… Ancestor chain ìƒì„± ì™„ë£Œ")

        # 4. ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ë¥˜ ë° ì²˜ë¦¬
        df_company = self._process_company_daily_logs(df)
        df_diary = self._process_habit_tracker_diary(df)
        df_weekly = self._process_weekly_reports(df)

        # 5. ëª¨ë“  ë¶„ë¥˜ ë¬¸ì„œ í†µí•©
        df_all = pd.concat([df_company, df_diary, df_weekly], ignore_index=True)

        # 6. ì›ë³¸ê³¼ ë³‘í•©
        df_merged = self._merge_with_original(df, df_all)

        # 6.5. General íƒ€ì…ì˜ ref_date ì±„ìš°ê¸° (created_timeì—ì„œ ì¶”ì¶œ)
        df_merged = self._fill_general_ref_dates(df_merged)

        # 6.6. MVP: General íƒ€ì… ë¬¸ì„œë¥¼ invalidë¡œ ë§ˆí‚¹
        df_merged = self._mark_general_as_invalid(df_merged)

        # 7. Cleaned documentsë¡œ ë³€í™˜
        cleaned_documents = self._to_cleaned_documents(df_merged)

        self.log(f"âœ… Notion ì „ì²˜ë¦¬ ì™„ë£Œ: {len(cleaned_documents)}ê±´")
        self.log("="*50)

        return cleaned_documents

    def _mark_invalid_documents(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë¬´íš¨ ë¬¸ì„œë¥¼ ë§ˆí‚¹í•©ë‹ˆë‹¤."""
        def is_invalid(row):
            title = str(row.get("title", "") or "").strip().lower()
            content = str(row.get("content", "") or "").strip()

            # ì œëª©ì´ ì—†ê±°ë‚˜ untitled
            if title in ["", "untitled", "ì œëª© ì—†ìŒ", "no title", "ì—†ìŒ"]:
                return True

            # ë‚´ìš©ì´ ì™„ì „íˆ ë¹„ì—ˆê±°ë‚˜ ê³µë°±ë§Œ
            if not content or re.fullmatch(r"[\s\n\t]*", content):
                return True

            # í…œí”Œë¦¿ë§Œ ìˆëŠ” ê²½ìš°
            if self.TEMPLATE_PATTERN.fullmatch(content):
                return True

            return False

        invalid_mask = df.apply(is_invalid, axis=1).astype(bool)
        df["is_valid"] = ~invalid_mask

        total = len(df)
        valid = int(df["is_valid"].sum())
        self.log(f"ğŸ“Š ì´ ë¬¸ì„œ {total}ê°œ ì¤‘ ìœ íš¨ {valid}ê°œ ({round(valid/total*100, 2)}%), ë¬´íš¨ {total-valid}ê°œ")

        return df

    def _process_company_daily_logs(self, df: pd.DataFrame) -> pd.DataFrame:
        """íšŒì‚¬ ì¼ì¼ì—…ë¬´ì •ë¦¬ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        df_company = filter_by_parent_level_and_title(df, 'ì¼ì¼ì—…ë¬´ì •ë¦¬', min_sub_depth=1)

        if df_company.empty:
            self.log("âš ï¸ íšŒì‚¬ ì¼ì¼ì—…ë¬´ì •ë¦¬ ë¬¸ì„œ ì—†ìŒ")
            return pd.DataFrame()

        # ref_date ì¶”ì¶œ
        df_company[["ref_date", "is_valid"]] = df_company.apply(
            lambda r: pd.Series(self._extract_ref_date(r["title"], r["ancestor_chain"])),
            axis=1
        )
        df_company["doc_type"] = "daily_log_company"

        valid_count = df_company['is_valid'].sum()
        null_count = df_company['ref_date'].isnull().sum()
        self.log(f"âœ… íšŒì‚¬ ì¼ì¼ì—…ë¬´ì •ë¦¬: ì´ {len(df_company)}ê°œ, ìœ íš¨ {valid_count}ê°œ, ref_date null {null_count}ê°œ")

        return df_company

    def _process_habit_tracker_diary(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìŠµê´€ íŠ¸ë˜ì»¤ ì¼ê¸° ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        df_diary = filter_by_ancestor_title(df, target_title='ìŠµê´€ ë¦¬ìŠ¤íŠ¸').copy()

        if df_diary.empty:
            self.log("âš ï¸ ìŠµê´€ íŠ¸ë˜ì»¤ ì¼ê¸° ë¬¸ì„œ ì—†ìŒ")
            return pd.DataFrame()

        # ìœ íš¨ì„± ê²€ì‚¬ (í…œí”Œë¦¿ë§Œ ìˆëŠ” ë¬¸ì„œ ì œì™¸)
        def is_valid_diary_text(text: str) -> bool:
            if not isinstance(text, str):
                return False
            stripped = text.strip()
            if stripped == "" or self.HABIT_TEMPLATE_PATTERN.match(stripped):
                return False
            return True

        df_diary["is_valid"] = df_diary["content"].apply(is_valid_diary_text)

        # created_time + 1ì¼ì„ ref_dateë¡œ ì‚¬ìš©
        df_diary = self._add_day_and_format(df_diary, time_column="created_time")
        df_diary['doc_type'] = 'diary'

        valid_count = df_diary["is_valid"].sum()
        self.log(f"âœ… ìŠµê´€ íŠ¸ë˜ì»¤: ì´ {len(df_diary)}ê°œ, ìœ íš¨ {valid_count}ê°œ")

        return df_diary

    def _process_weekly_reports(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì£¼ê°„ì—…ë¬´ì •ë¦¬ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        df_weekly = filter_by_ancestor_title(df, target_title='ì£¼ê°„ì—…ë¬´ì •ë¦¬ ')

        if df_weekly.empty:
            self.log("âš ï¸ ì£¼ê°„ì—…ë¬´ì •ë¦¬ ë¬¸ì„œ ì—†ìŒ")
            return pd.DataFrame()

        # ref_date ì¶”ì¶œ (titleì—ì„œ ë¨¼ì € ì‹œë„, ì‹¤íŒ¨ ì‹œ ancestor chainì—ì„œ ì¶”ì¶œ)
        df_weekly[["ref_date", "week_start_date", "week_end_date"]] = df_weekly.apply(
            lambda row: pd.Series(self._extract_weekly_dates(row)),
            axis=1
        )

        df_weekly["doc_type"] = "weekly_report"
        df_weekly["is_valid"] = True  # ì£¼ê°„ë³´ê³ ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìœ íš¨

        valid_count = df_weekly["is_valid"].sum()
        null_count = df_weekly["ref_date"].isnull().sum()
        self.log(f"âœ… ì£¼ê°„ì—…ë¬´ì •ë¦¬: ì´ {len(df_weekly)}ê°œ, ìœ íš¨ {valid_count}ê°œ, ref_date null {null_count}ê°œ")

        return df_weekly

    def _extract_weekly_dates(self, row: pd.Series) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Weekly reportì˜ ë‚ ì§œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Returns:
            (ref_date, week_start_date, week_end_date) íŠœí”Œ
        """
        title = row.get("title", "")
        ancestor_chain = row.get("ancestor_chain", "")

        # 1. Titleì—ì„œ ë¨¼ì € ì‹œë„
        week_start, week_end = extract_week_range_from_title(title)
        if week_start is not None and week_end is not None:
            ref_date = week_start.strftime('%Y-%m-%d')
            week_start_str = week_start.strftime('%Y-%m-%d')
            week_end_str = week_end.strftime('%Y-%m-%d')
            return (ref_date, week_start_str, week_end_str)

        # 2. Ancestor chainì—ì„œ ì‹œë„ (ë¶€ëª¨ ë¬¸ì„œì˜ ë‚ ì§œ ì •ë³´ ìƒì†)
        if ancestor_chain:
            # Ancestor chainì„ ë¶„í• í•˜ì—¬ ê° ë ˆë²¨ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
            nodes = [n.strip() for n in ancestor_chain.split('â†’') if n.strip()]
            # ì—­ìˆœìœ¼ë¡œ (ìì‹ -> ë¶€ëª¨) íƒìƒ‰
            for node in reversed(nodes):
                week_start, week_end = extract_week_range_from_title(node)
                if week_start is not None and week_end is not None:
                    ref_date = week_start.strftime('%Y-%m-%d')
                    week_start_str = week_start.strftime('%Y-%m-%d')
                    week_end_str = week_end.strftime('%Y-%m-%d')
                    return (ref_date, week_start_str, week_end_str)

        # 3. ì¶”ì¶œ ì‹¤íŒ¨
        return (None, None, None)

    def _extract_ref_date(
        self,
        title: str,
        ancestor_chain: Optional[str] = None
    ) -> Tuple[Optional[str], bool]:
        """
        Title ë˜ëŠ” ancestor chainì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Returns:
            (ref_date, is_valid) íŠœí”Œ
        """
        def _extract_from_text(text: str) -> Optional[str]:
            pattern = r'(\d{4})[^\d]?(\d{1,2})[^\d]?(\d{1,2})(?:[^\d]?[ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼])?'
            m = re.search(pattern, text)
            if m:
                from .utils import normalize_date
                return normalize_date(*m.groups())
            return None

        # Titleì—ì„œ ë¨¼ì € ì‹œë„
        if title:
            title_clean = clean_text(title)
            result = _extract_from_text(title_clean)
            if result:
                return result, True

        # Ancestor chainì˜ ë§ˆì§€ë§‰ ë…¸ë“œì—ì„œ ì‹œë„
        if ancestor_chain:
            ancestor_clean = clean_text(ancestor_chain)
            nodes = [n.strip() for n in ancestor_clean.split('â†’') if n.strip()]
            if nodes:
                result = _extract_from_text(nodes[-1])
                if result:
                    return result, True

            # ì „ì²´ ancestor chainì—ì„œ ì‹œë„
            result = _extract_from_text(ancestor_clean)
            if result:
                return result, True

        return None, False

    def _add_day_and_format(
        self,
        df: pd.DataFrame,
        time_column: str = 'created_time'
    ) -> pd.DataFrame:
        """íŠ¹ì • ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì— í•˜ë£¨ë¥¼ ë”í•˜ê³  ref_date ìƒì„±"""
        df_result = df.copy()
        df_result[time_column] = pd.to_datetime(df_result[time_column], errors='coerce')
        temp_date_col = '__temp_date_dt__'
        df_result[temp_date_col] = df_result[time_column] + pd.Timedelta('1 day')
        df_result['ref_date'] = df_result[temp_date_col].dt.strftime('%Y-%m-%d')
        df_result = df_result.drop(columns=[temp_date_col])
        return df_result

    def _merge_with_original(
        self,
        df_original: pd.DataFrame,
        df_classified: pd.DataFrame
    ) -> pd.DataFrame:
        """ë¶„ë¥˜ëœ ë¬¸ì„œë¥¼ ì›ë³¸ê³¼ ë³‘í•©í•©ë‹ˆë‹¤."""
        # ë³‘í•©í•  ì»¬ëŸ¼ ë™ì  ê²°ì • (week_start_date, week_end_dateê°€ ìˆìœ¼ë©´ í¬í•¨)
        merge_columns = ["id", "doc_type", "ref_date", "is_valid"]
        if "week_start_date" in df_classified.columns:
            merge_columns.append("week_start_date")
        if "week_end_date" in df_classified.columns:
            merge_columns.append("week_end_date")

        df_merged = df_original.merge(
            df_classified[merge_columns],
            on="id",
            how="left",
            suffixes=("", "_classified")
        )

        # ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë¬¸ì„œëŠ” generalë¡œ
        df_merged["doc_type"] = df_merged["doc_type"].fillna("general")
        df_merged["is_valid"] = df_merged["is_valid"].fillna(df_merged["is_valid_classified"])
        df_merged.drop(columns=["is_valid_classified"], inplace=True, errors='ignore')

        return df_merged

    def _fill_general_ref_dates(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        General íƒ€ì… ë¬¸ì„œì˜ ref_dateë¥¼ created_timeì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        doc_typeì´ 'general'ì´ê³  ref_dateê°€ nullì¸ ê²½ìš°ì—ë§Œ ì ìš©í•©ë‹ˆë‹¤.

        Args:
            df: ë³‘í•©ëœ DataFrame

        Returns:
            ref_dateê°€ ì±„ì›Œì§„ DataFrame
        """
        # General íƒ€ì…ì´ë©´ì„œ ref_dateê°€ ì—†ëŠ” ë¬¸ì„œ í•„í„°ë§
        general_mask = (df['doc_type'] == 'general') & (df['ref_date'].isnull())
        general_count = general_mask.sum()

        if general_count > 0:
            # created_timeì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ (ì‹œê°„ ë¶€ë¶„ ì œê±°)
            df.loc[general_mask, 'ref_date'] = pd.to_datetime(
                df.loc[general_mask, 'created_time'],
                errors='coerce'
            ).dt.strftime('%Y-%m-%d')

            filled_count = df.loc[general_mask, 'ref_date'].notna().sum()
            self.log(f"âœ… General íƒ€ì… ref_date ì±„ìš°ê¸°: {filled_count}/{general_count}ê±´")

        return df

    def _mark_general_as_invalid(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        MVP: General íƒ€ì… ë¬¸ì„œë¥¼ invalidë¡œ ë§ˆí‚¹í•©ë‹ˆë‹¤.

        ë‚˜ì¤‘ì— general íƒ€ì…ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì´ ë©”ì„œë“œë¥¼ ì œê±°í•˜ë©´ ë©ë‹ˆë‹¤.

        Args:
            df: ë³‘í•©ëœ DataFrame

        Returns:
            general íƒ€ì…ì´ invalidë¡œ ë§ˆí‚¹ëœ DataFrame
        """
        general_mask = df['doc_type'] == 'general'
        general_count = general_mask.sum()

        if general_count > 0:
            df.loc[general_mask, 'is_valid'] = False
            self.log(f"ğŸš« MVP: General íƒ€ì… {general_count}ê±´ì„ invalidë¡œ ë§ˆí‚¹")

        return df

    def _to_cleaned_documents(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """
        DataFrameì„ CleanedNotionDocument dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        cleaned_docs = []

        for _, row in df.iterrows():
            # is_validê°€ Falseì¸ ê²½ìš° ìŠ¤í‚µ (ë˜ëŠ” í¬í•¨í• ì§€ëŠ” ì„ íƒ)
            if not row.get('is_valid', True):
                continue

            # ìì—°ì–´ content ìƒì„± (ì œëª© + ê²½ë¡œ + ë³¸ë¬¸)
            content = self._synthesize_notion_content(row)

            # Metadata êµ¬ì„±
            metadata = {
                "notion_page_id": row.get('notion_page_id', ''),
                "title": row.get('title', ''),
                "url": row.get('url', ''),
                "ancestor_chain": row.get('ancestor_chain', ''),
                "created_time": row['created_time'].isoformat() if pd.notna(row.get('created_time')) else None,
                "last_edited_time": row['last_edited_time'].isoformat() if pd.notna(row.get('last_edited_time')) else None,
                "properties": row.get('properties', {}),
                "has_images": bool(row.get('image_gridfs_ids')),
                "image_gridfs_ids": row.get('image_gridfs_ids', []) or []
            }

            # Weekly reportì¸ ê²½ìš° ì£¼ê°„ ë²”ìœ„ ì •ë³´ ì¶”ê°€
            if row.get('doc_type') == 'weekly_report':
                if pd.notna(row.get('week_start_date')):
                    metadata['week_start_date'] = row['week_start_date']
                if pd.notna(row.get('week_end_date')):
                    metadata['week_end_date'] = row['week_end_date']

            # CleanedNotionDocument dict ìƒì„±
            cleaned_doc = {
                "original_id": str(row['id']),
                "content": content,
                "ref_date": row.get('ref_date') or '',
                "platform": "notion",
                "doc_type": row.get('doc_type', 'general'),
                "author_id": str(row['author_id']),
                "author_full_name": row['author_full_name'],
                "is_valid": bool(row.get('is_valid', True)),
                "metadata": metadata
            }

            cleaned_docs.append(cleaned_doc)

        return cleaned_docs

    def _synthesize_notion_content(self, row: pd.Series) -> str:
        """
        Notion í˜ì´ì§€ë¥¼ ìì—°ì–´ contentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        êµ¬ì„±:
        - ì œëª©
        - ê²½ë¡œ (ancestor chain)
        - ë³¸ë¬¸ (ë§ˆí¬ë‹¤ìš´)
        """
        title = row.get('title', 'ì œëª© ì—†ìŒ')
        ancestor_chain = row.get('ancestor_chain', '')
        body = row.get('content', '')

        content_parts = []

        # ì œëª©
        content_parts.append(f"ì œëª©: {title}")

        # ê²½ë¡œ
        if ancestor_chain:
            content_parts.append(f"ê²½ë¡œ: {ancestor_chain}")

        # êµ¬ë¶„ì„ 
        content_parts.append("\n---\n")

        # ë³¸ë¬¸
        if body and body.strip():
            content_parts.append(body.strip())
        else:
            content_parts.append("(ë‚´ìš© ì—†ìŒ)")

        return "\n".join(content_parts)
