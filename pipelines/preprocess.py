"""
ì „ì²˜ë¦¬ ëª¨ë“ˆ.

MongoDBì˜ ì›ë³¸ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ ì „ì²˜ë¦¬ í›„ CleanedDocumentsë¡œ ì €ì¥í•©ë‹ˆë‹¤.
ì¦ë¶„ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
"""

from datetime import datetime
from typing import Dict, Tuple

import pandas as pd
from loguru import logger

from llm_engineering.domain.documents import (
    CalendarDocument,
    GoogleCalendarDocument,
    NotionPageDocument,
    NaverPostDocument,
)
from llm_engineering.domain.cleaned_documents import (
    CleanedCalendarDocument,
    CleanedNotionDocument,
    CleanedNaverDocument,
)
from llm_engineering.application.preprocessing import PreprocessorDispatcher


def get_processed_document_map(cleaned_doc_class) -> Dict[str, datetime]:
    """
    Cleaned documentsì—ì„œ original_id -> processed_at ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        cleaned_doc_class: CleanedDocument í´ë˜ìŠ¤ (ì˜ˆ: CleanedNotionDocument)

    Returns:
        {original_id: processed_at} ë”•ì…”ë„ˆë¦¬
    """
    try:
        cleaned_docs = cleaned_doc_class.bulk_find()
        return {
            str(doc.original_id): doc.processed_at
            for doc in cleaned_docs
        }
    except Exception:
        return {}


def filter_documents_to_process(
    df: pd.DataFrame,
    processed_map: Dict[str, datetime],
    time_column: str = 'last_edited_time'
) -> pd.DataFrame:
    """
    ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.

    ì¡°ê±´:
    1. Cleaned ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° (ìƒˆ ë¬¸ì„œ)
    2. ì›ë³¸ ë¬¸ì„œì˜ time_column > cleaned ë¬¸ì„œì˜ processed_at (ë³€ê²½ëœ ë¬¸ì„œ)

    Args:
        df: ì›ë³¸ ë¬¸ì„œ DataFrame
        processed_map: {original_id: processed_at} ë§¤í•‘
        time_column: ë¹„êµí•  ì‹œê°„ ì»¬ëŸ¼ (ê¸°ë³¸: last_edited_time)

    Returns:
        í•„í„°ë§ëœ DataFrame
    """
    if df.empty:
        return df

    def needs_processing(row):
        doc_id = str(row['id'])

        # 1. Cleaned ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬ í•„ìš”
        if doc_id not in processed_map:
            return True

        # 2. ì‹œê°„ ë¹„êµ (ì›ë³¸ì´ ë” ìµœì‹ ì´ë©´ ì²˜ë¦¬ í•„ìš”)
        original_time = row.get(time_column)
        if pd.notna(original_time):
            processed_time = processed_map[doc_id]
            # datetime ê°ì²´ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            if not isinstance(original_time, datetime):
                original_time = pd.to_datetime(original_time)
            return original_time > processed_time

        return False

    mask = df.apply(needs_processing, axis=1)
    return df[mask].copy()


def load_raw_data(incremental: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    MongoDBì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        incremental: Trueì´ë©´ ë³€ê²½ëœ ë¬¸ì„œë§Œ, Falseì´ë©´ ì „ì²´ ë¬¸ì„œ

    Returns:
        (df_calendar, df_google_calendar, df_notion, df_naver) íŠœí”Œ
    """
    logger.info("=" * 70)
    logger.info(f"ğŸ“¥ ì›ë³¸ ë°ì´í„° ë¡œë”© ì¤‘... (ì¦ë¶„ ì²˜ë¦¬: {'ON' if incremental else 'OFF'})")
    logger.info("=" * 70)

    # Calendar ë°ì´í„° ë¡œë“œ
    try:
        calendar_docs = CalendarDocument.bulk_find()
        df_calendar = pd.DataFrame([doc.model_dump() for doc in calendar_docs])
        total_calendar = len(df_calendar)

        if incremental and not df_calendar.empty:
            processed_map = get_processed_document_map(CleanedCalendarDocument)
            df_calendar = filter_documents_to_process(
                df_calendar,
                processed_map,
                time_column='end_datetime'
            )
            logger.info(f"âœ… Calendar: {len(df_calendar)}ê±´ ì²˜ë¦¬ í•„ìš” (ì „ì²´ {total_calendar}ê±´)")
        else:
            logger.info(f"âœ… Calendar: {len(df_calendar)}ê±´ ë¡œë“œ")
    except Exception as e:
        logger.error(f"âŒ Calendar ë¡œë“œ ì‹¤íŒ¨: {e}")
        df_calendar = pd.DataFrame()

    # Google Calendar ë°ì´í„° ë¡œë“œ
    try:
        google_calendar_docs = GoogleCalendarDocument.bulk_find(is_deleted=False)
        df_google_calendar = pd.DataFrame([doc.model_dump() for doc in google_calendar_docs])
        total_google_calendar = len(df_google_calendar)

        if incremental and not df_google_calendar.empty:
            processed_map = get_processed_document_map(CleanedCalendarDocument)
            df_google_calendar = filter_documents_to_process(
                df_google_calendar,
                processed_map,
                time_column='last_synced_at'
            )
            logger.info(f"âœ… Google Calendar: {len(df_google_calendar)}ê±´ ì²˜ë¦¬ í•„ìš” (ì „ì²´ {total_google_calendar}ê±´)")
        else:
            logger.info(f"âœ… Google Calendar: {len(df_google_calendar)}ê±´ ë¡œë“œ")
    except Exception as e:
        logger.error(f"âŒ Google Calendar ë¡œë“œ ì‹¤íŒ¨: {e}")
        df_google_calendar = pd.DataFrame()

    # Notion ë°ì´í„° ë¡œë“œ
    try:
        notion_docs = NotionPageDocument.bulk_find()
        df_notion = pd.DataFrame([doc.model_dump() for doc in notion_docs])
        total_notion = len(df_notion)

        if incremental and not df_notion.empty:
            processed_map = get_processed_document_map(CleanedNotionDocument)
            df_notion = filter_documents_to_process(
                df_notion,
                processed_map,
                time_column='last_edited_time'
            )
            logger.info(f"âœ… Notion: {len(df_notion)}ê±´ ì²˜ë¦¬ í•„ìš” (ì „ì²´ {total_notion}ê±´)")
        else:
            logger.info(f"âœ… Notion: {len(df_notion)}ê±´ ë¡œë“œ")
    except Exception as e:
        logger.error(f"âŒ Notion ë¡œë“œ ì‹¤íŒ¨: {e}")
        df_notion = pd.DataFrame()

    # Naver ë°ì´í„° ë¡œë“œ
    try:
        naver_docs = NaverPostDocument.bulk_find()
        df_naver = pd.DataFrame([doc.model_dump() for doc in naver_docs])
        total_naver = len(df_naver)

        if incremental and not df_naver.empty:
            processed_map = get_processed_document_map(CleanedNaverDocument)
            df_naver = filter_documents_to_process(
                df_naver,
                processed_map,
                time_column='published_at'
            )
            logger.info(f"âœ… Naver: {len(df_naver)}ê±´ ì²˜ë¦¬ í•„ìš” (ì „ì²´ {total_naver}ê±´)")
        else:
            logger.info(f"âœ… Naver: {len(df_naver)}ê±´ ë¡œë“œ")
    except Exception as e:
        logger.error(f"âŒ Naver ë¡œë“œ ì‹¤íŒ¨: {e}")
        df_naver = pd.DataFrame()

    return df_calendar, df_google_calendar, df_notion, df_naver


def preprocess_data(
    df_calendar: pd.DataFrame,
    df_google_calendar: pd.DataFrame,
    df_notion: pd.DataFrame,
    df_naver: pd.DataFrame,
    verbose: bool = True
) -> dict:
    """
    ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        df_calendar: Calendar DataFrame
        df_google_calendar: Google Calendar DataFrame
        df_notion: Notion DataFrame
        df_naver: Naver DataFrame
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ {source: cleaned_docs_list}
    """
    logger.info("=" * 70)
    logger.info("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    logger.info("=" * 70)

    dispatcher = PreprocessorDispatcher(verbose=verbose)

    # ì„¤ì •
    configs = {
        "calendar": {
            "category_rename_rules": [
                {"old": "yoonhs010@gmail.com", "new": "êµ¬ê¸€ìº˜ë¦°ë”", "before_date": "2025-10-24"},
                {"old": "ìœ ì§€ / ì •ë¦¬", "new": "ì´ë™", "before_date": "2025-09-27"}
            ]
        },
        "google_calendar": {},
        "naver": {
            "filter_categories": ["ì¼ì¼í”¼ë“œë°±"]
        }
    }

    # ì „ì²˜ë¦¬ ì‹¤í–‰
    all_cleaned = dispatcher.preprocess_all(
        {
            "calendar": df_calendar,
            "google_calendar": df_google_calendar,
            "notion": df_notion,
            "naver": df_naver
        },
        configs=configs
    )

    return all_cleaned


def save_cleaned_documents(cleaned_data: dict) -> dict:
    """
    Cleaned documentsë¥¼ MongoDBì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        cleaned_data: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬

    Returns:
        ì €ì¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ {source: {modified: int, upserted: int}}
    """
    logger.info("=" * 70)
    logger.info("ğŸ’¾ Cleaned Documents ì €ì¥ ì¤‘ (Bulk Upsert)...")
    logger.info("=" * 70)

    results = {}

    def prepare_docs_for_upsert(doc_class, docs):
        """Upsertë¥¼ ìœ„í•´ ë¬¸ì„œ ì¤€ë¹„: ê¸°ì¡´ ë¬¸ì„œì˜ _idë¥¼ ì°¾ì•„ì„œ í• ë‹¹"""
        existing_docs = doc_class.bulk_find()
        existing_map = {str(doc.original_id): doc.id for doc in existing_docs}

        for doc in docs:
            original_id_str = str(doc.original_id)
            if original_id_str in existing_map:
                doc.id = existing_map[original_id_str]

        return docs

    # Calendar
    if cleaned_data.get("calendar"):
        calendar_docs = [
            CleanedCalendarDocument(**doc) for doc in cleaned_data["calendar"]
        ]
        if calendar_docs:
            calendar_docs = prepare_docs_for_upsert(CleanedCalendarDocument, calendar_docs)
            result = CleanedCalendarDocument.bulk_upsert(calendar_docs, match_field="_id")
            results["calendar"] = result
            logger.info(f"âœ… Calendar: {len(calendar_docs)}ê±´ ì²˜ë¦¬ "
                       f"(ìˆ˜ì •: {result['modified']}, ì‹ ê·œ: {result['upserted']})")

    # Google Calendar
    if cleaned_data.get("google_calendar"):
        google_calendar_docs = [
            CleanedCalendarDocument(**doc) for doc in cleaned_data["google_calendar"]
        ]
        if google_calendar_docs:
            google_calendar_docs = prepare_docs_for_upsert(CleanedCalendarDocument, google_calendar_docs)
            result = CleanedCalendarDocument.bulk_upsert(google_calendar_docs, match_field="_id")
            results["google_calendar"] = result
            logger.info(f"âœ… Google Calendar: {len(google_calendar_docs)}ê±´ ì²˜ë¦¬ "
                       f"(ìˆ˜ì •: {result['modified']}, ì‹ ê·œ: {result['upserted']})")

    # Notion
    if cleaned_data.get("notion"):
        notion_docs = [
            CleanedNotionDocument(**doc) for doc in cleaned_data["notion"]
        ]
        if notion_docs:
            notion_docs = prepare_docs_for_upsert(CleanedNotionDocument, notion_docs)
            result = CleanedNotionDocument.bulk_upsert(notion_docs, match_field="_id")
            results["notion"] = result
            logger.info(f"âœ… Notion: {len(notion_docs)}ê±´ ì²˜ë¦¬ "
                       f"(ìˆ˜ì •: {result['modified']}, ì‹ ê·œ: {result['upserted']})")

    # Naver
    if cleaned_data.get("naver"):
        naver_docs = [
            CleanedNaverDocument(**doc) for doc in cleaned_data["naver"]
        ]
        if naver_docs:
            naver_docs = prepare_docs_for_upsert(CleanedNaverDocument, naver_docs)
            result = CleanedNaverDocument.bulk_upsert(naver_docs, match_field="_id")
            results["naver"] = result
            logger.info(f"âœ… Naver: {len(naver_docs)}ê±´ ì²˜ë¦¬ "
                       f"(ìˆ˜ì •: {result['modified']}, ì‹ ê·œ: {result['upserted']})")

    return results


def run_preprocessing(
    incremental: bool = True,
    save: bool = True,
    verbose: bool = True
) -> dict:
    """
    ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        incremental: ì¦ë¶„ ì²˜ë¦¬ ì—¬ë¶€ (ê¸°ë³¸: True)
        save: MongoDBì— ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸: True)
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸: True)

    Returns:
        ì „ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    logger.info("=" * 70)
    logger.info(f"ğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ({'ì¦ë¶„ ì²˜ë¦¬' if incremental else 'ì „ì²´ ì¬ì²˜ë¦¬'})")
    logger.info("=" * 70)

    # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ
    df_calendar, df_google_calendar, df_notion, df_naver = load_raw_data(incremental=incremental)

    # 2. ì „ì²˜ë¦¬
    cleaned_data = preprocess_data(
        df_calendar, df_google_calendar, df_notion, df_naver, verbose=verbose
    )

    # 3. ì €ì¥
    if save:
        save_cleaned_documents(cleaned_data)
        logger.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ ë° ì €ì¥ë¨")
    else:
        logger.info("âš ï¸ ì „ì²˜ë¦¬ ì™„ë£Œ (ì €ì¥ ì•ˆí•¨)")

    # 4. í†µê³„
    total_cleaned = sum(len(docs) for docs in cleaned_data.values())
    logger.info(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ ë¬¸ì„œ: {total_cleaned}ê±´")
    for platform, docs in cleaned_data.items():
        if docs:
            logger.info(f"   - {platform}: {len(docs)}ê±´")

    return cleaned_data
